{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-20T15:53:24.572495Z","iopub.execute_input":"2023-04-20T15:53:24.572879Z","iopub.status.idle":"2023-04-20T15:53:24.577515Z","shell.execute_reply.started":"2023-04-20T15:53:24.572843Z","shell.execute_reply":"2023-04-20T15:53:24.576476Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-20T15:53:24.583175Z","iopub.execute_input":"2023-04-20T15:53:24.583906Z","iopub.status.idle":"2023-04-20T15:53:24.591804Z","shell.execute_reply.started":"2023-04-20T15:53:24.583878Z","shell.execute_reply":"2023-04-20T15:53:24.590550Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"# pip install oct2py\n#!apt-get install octave -y","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:53:24.593800Z","iopub.execute_input":"2023-04-20T15:53:24.594535Z","iopub.status.idle":"2023-04-20T15:53:24.605120Z","shell.execute_reply.started":"2023-04-20T15:53:24.594496Z","shell.execute_reply":"2023-04-20T15:53:24.604163Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:53:24.609240Z","iopub.execute_input":"2023-04-20T15:53:24.609530Z","iopub.status.idle":"2023-04-20T15:53:24.617151Z","shell.execute_reply.started":"2023-04-20T15:53:24.609505Z","shell.execute_reply":"2023-04-20T15:53:24.616108Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-20T15:53:24.619564Z","iopub.execute_input":"2023-04-20T15:53:24.620038Z","iopub.status.idle":"2023-04-20T15:53:24.630959Z","shell.execute_reply.started":"2023-04-20T15:53:24.620003Z","shell.execute_reply":"2023-04-20T15:53:24.629657Z"},"trusted":true},"execution_count":165,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:53:24.632597Z","iopub.execute_input":"2023-04-20T15:53:24.633138Z","iopub.status.idle":"2023-04-20T15:53:24.643651Z","shell.execute_reply.started":"2023-04-20T15:53:24.633102Z","shell.execute_reply":"2023-04-20T15:53:24.642380Z"},"trusted":true},"execution_count":166,"outputs":[{"execution_count":166,"output_type":"execute_result","data":{"text/plain":"['EEGPT_Models', '.virtual_documents', '__notebook_source__.ipynb']"},"metadata":{}}]},{"cell_type":"code","source":"datatype = 'eeg'","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:53:24.644995Z","iopub.execute_input":"2023-04-20T15:53:24.645738Z","iopub.status.idle":"2023-04-20T15:53:24.650749Z","shell.execute_reply.started":"2023-04-20T15:53:24.645701Z","shell.execute_reply":"2023-04-20T15:53:24.649523Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"if datatype == 'eeg':\n    sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n    # sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n    # data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\nelif datatype == 'ica':\n    sub01 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub01.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub02.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub03.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub04.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub05.mat')\n    sub06 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub06.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub07.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub08.mat')\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-20T15:53:24.652514Z","iopub.execute_input":"2023-04-20T15:53:24.653319Z","iopub.status.idle":"2023-04-20T15:53:26.895927Z","shell.execute_reply.started":"2023-04-20T15:53:24.653275Z","shell.execute_reply":"2023-04-20T15:53:26.894902Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-20T15:53:26.899823Z","iopub.execute_input":"2023-04-20T15:53:26.900134Z","iopub.status.idle":"2023-04-20T15:53:26.908407Z","shell.execute_reply.started":"2023-04-20T15:53:26.900105Z","shell.execute_reply":"2023-04-20T15:53:26.907397Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"class testEEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(testEEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=17, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=10,stride=10)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=15,stride=1,padding=\"same\") # output should be \n    self.grp_norm_s = nn.GroupNorm(eeg_channels//6,eeg_channels)\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=time_len//10,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=time_len//10,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=False, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1))    \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels//2,max_length=1500)\n#     self.Transf1_t = EncoderTransformer(inSize=time_len,outSize=4,numLayers=3,hiddenSize=1,numHeads=6,dropout=0.01)\n    self.Transf1_t = EncoderTransformer(inSize=eeg_channels//2,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n    # Build Fully Connected Path\n    self.fc1 = nn.Linear(1260,1)\n\n  def forward(self, x):\n    # Spatial Pass\n    x_s = self.Conv1_s(x)\n#     print('x_s conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x_s avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x_s conv21: ',x_s.shape)\n    x_s = self.grp_norm_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s tf1: ',x_s.shape)\n    \n    # Temporal Pass\n    x_t = self.dwconv1_t(x)\n#     print('x_t conv1: ',x_t.shape)\n    x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    x_t = x_t.permute(0,2,1) # transpose to present time wise vectors to transformer encoder    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t tf1: ',x_t.shape)\n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n#     print('x_cat: ',x_cat.shape)\n    # Output Pass: Fully Connected into Softmax\n    x = self.fc1(x_cat)\n    x = torch.log_softmax(x,dim=1)\n    return x\n\nclass EEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(EEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n    self.AvgPool2_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n    self.AvgPool3_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=100,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=100,outSize=5,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=True, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1)) \n    self.conv2_t = nn.Conv1d(in_channels=eeg_channels//2,out_channels=eeg_channels//2, kernel_size=3, stride=1, bias = False, padding='same')\n    self.AvgPool2_t = nn.AvgPool2d(kernel_size=(2,1)) \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=60,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=60,outSize=5,numLayers=5,hiddenSize=5,numHeads=10,dropout=0.001)\n    # Build Fully Connected Path\n    if datatype == 'eeg':\n        self.fc1 = nn.Linear(1260,1)\n    elif datatype == 'ica':\n        self.fc1 = nn.Linear(1220,1)\n        \n\n  def forward(self, x):\n    # Spatial Pass\n    \n    x = x.to(torch.float32)\n#     print('x: ',x.shape)\n    x_s = self.Conv1_s(x)\n#     print('x conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x conv2: ',x_s.shape)\n#     x_s = self.AvgPool2_s(x_s)\n#     print('x avg2: ',x_s.shape)\n#     x_s = self.Conv3_s(x_s)\n#     print('x conv3: ',x_s.shape)\n#     x_s = self.AvgPool3_s(x_s)\n#     x_s = self.Conv4_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s_transf: ', x_s.shape)\n    \n    # Temporal Pass\n    #x_t = self.dwconv1_t(x)\n    #print('x_t conv1: ',x_t.shape)\n    #x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    #x_t = self.conv2_t(x_t)\n#     print('x_t conv2: ',x_t.shape)\n    #x_t = self.AvgPool2_t(x_t)\n#     print('x_t avg2: ',x_t.shape)\n    x_t = x.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n#     print('x_t avg1_permute: ',x_t.shape)    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t_transf: ', x_t.shape)\n    \n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n#     print('x_t transf1_perm: ',x_t.shape)\n#     print('x_s transf1_perm: ',x_s.shape)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n    # Output Pass: Fully Connected into Softmax\n#     print('x cat: ',x_cat.shape)\n    x = self.fc1(x_cat)\n#     print('x fc1: ',x.shape)\n    x = torch.log_softmax(x,dim=1)\n#     print('x softmax: ',x.shape)\n    return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-20T16:01:10.252663Z","iopub.execute_input":"2023-04-20T16:01:10.253056Z","iopub.status.idle":"2023-04-20T16:01:10.283943Z","shell.execute_reply.started":"2023-04-20T16:01:10.253023Z","shell.execute_reply":"2023-04-20T16:01:10.282857Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))\n\ndef mas2565_ICA(tensor):\n    pass\n    #return ICA_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:01:10.512030Z","iopub.execute_input":"2023-04-20T16:01:10.512539Z","iopub.status.idle":"2023-04-20T16:01:10.526030Z","shell.execute_reply.started":"2023-04-20T16:01:10.512492Z","shell.execute_reply":"2023-04-20T16:01:10.524868Z"},"trusted":true},"execution_count":228,"outputs":[]},{"cell_type":"code","source":"#print(data['sub01']['X_EEG_TRAIN'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:01:10.767823Z","iopub.execute_input":"2023-04-20T16:01:10.768806Z","iopub.status.idle":"2023-04-20T16:01:10.776386Z","shell.execute_reply.started":"2023-04-20T16:01:10.768759Z","shell.execute_reply":"2023-04-20T16:01:10.775222Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n    if i != 5:\n        subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n        subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n    for j in range(np.size(subSetY,0)):   \n        #print(np.shape(subSetX)[])\n        subx = subSetX[:,:,j]\n\n        #subx = mas2565_ICA(subx)\n        subx = mas2565_normalize(subx)\n        subx = mas2565_filter(subx)\n\n        #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n        #subx = noise(subx)\n        subx = torch.Tensor(subx)\n        #subx = mas2565_filter(subx)\n        #print(np.shape(subx))\n        suby = subSetY[j,:]\n        # miniSet = EEGData(subx,suby)\n        # print(np.shape(miniSet.y))\n        X.append(subx)\n        Y.append(suby)\n\n\n        # DEBUGGING PRINTS\n        #print(np.size(subSetY,0))\n        #print(np.shape(subSetX))\n        #print(np.shape(subSetY))\n        #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=3,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=3,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-20T16:01:11.062757Z","iopub.execute_input":"2023-04-20T16:01:11.063273Z","iopub.status.idle":"2023-04-20T16:01:12.582120Z","shell.execute_reply.started":"2023-04-20T16:01:11.063236Z","shell.execute_reply":"2023-04-20T16:01:12.580944Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = testEEGPT(eeg_channels=60, time_len=1200)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.0001)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-20T16:01:12.584772Z","iopub.execute_input":"2023-04-20T16:01:12.585200Z","iopub.status.idle":"2023-04-20T16:01:12.626485Z","shell.execute_reply.started":"2023-04-20T16:01:12.585163Z","shell.execute_reply":"2023-04-20T16:01:12.625608Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-20T16:01:12.629136Z","iopub.execute_input":"2023-04-20T16:01:12.629760Z","iopub.status.idle":"2023-04-20T16:01:12.637553Z","shell.execute_reply.started":"2023-04-20T16:01:12.629722Z","shell.execute_reply":"2023-04-20T16:01:12.636378Z"},"trusted":true},"execution_count":232,"outputs":[{"name":"stdout","text":"number of model params:  812281\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\nEPOCHS = 50\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode\n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-20T16:01:12.639938Z","iopub.execute_input":"2023-04-20T16:01:12.640396Z","iopub.status.idle":"2023-04-20T16:07:09.752211Z","shell.execute_reply.started":"2023-04-20T16:01:12.640356Z","shell.execute_reply":"2023-04-20T16:07:09.751037Z"},"trusted":true},"execution_count":233,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 0.9299752817799648 | validation_loss: 0.8786472082138062\nEpoch: 1 | train_loss: 0.7169848006839553 | validation_loss: 0.8589332699775696\nEpoch: 2 | train_loss: 0.7241389115030566 | validation_loss: 0.8423515558242798\nEpoch: 3 | train_loss: 0.716096531910201 | validation_loss: 0.7371622920036316\nEpoch: 4 | train_loss: 0.7099691076825062 | validation_loss: 0.5621458292007446\nEpoch: 5 | train_loss: 0.703067095639805 | validation_loss: 0.4211162328720093\nEpoch: 6 | train_loss: 0.7049229325105747 | validation_loss: 0.6992634534835815\nEpoch: 7 | train_loss: 0.7110978911320368 | validation_loss: 0.9379544854164124\nEpoch: 8 | train_loss: 0.7036428594340881 | validation_loss: 0.6212456822395325\nEpoch: 9 | train_loss: 0.7016750698288282 | validation_loss: 0.6528626680374146\nEpoch: 10 | train_loss: 0.7044431287795305 | validation_loss: 0.7235409617424011\nEpoch: 11 | train_loss: 0.7012467555080851 | validation_loss: 0.47408491373062134\nEpoch: 12 | train_loss: 0.6888896947105726 | validation_loss: 0.8564143776893616\nEpoch: 13 | train_loss: 0.6657642172649503 | validation_loss: 0.25501924753189087\nEpoch: 14 | train_loss: 0.6580668681611618 | validation_loss: 0.35648468136787415\nEpoch: 15 | train_loss: 0.6770950589949886 | validation_loss: 0.3630141317844391\nEpoch: 16 | train_loss: 0.6527675309528908 | validation_loss: 0.33456817269325256\nEpoch: 17 | train_loss: 0.6667339727282524 | validation_loss: 0.43191567063331604\nEpoch: 18 | train_loss: 0.6350212117346624 | validation_loss: 0.19803611934185028\nEpoch: 19 | train_loss: 0.6156116612255573 | validation_loss: 0.7550138235092163\nEpoch: 20 | train_loss: 0.5530069673744341 | validation_loss: 0.6718716621398926\nEpoch: 21 | train_loss: 0.5430230059816191 | validation_loss: 0.2108331173658371\nEpoch: 22 | train_loss: 0.5468024097693464 | validation_loss: 0.8794548511505127\nEpoch: 23 | train_loss: 0.5258816375086705 | validation_loss: 0.6871954798698425\nEpoch: 24 | train_loss: 0.5287070569271842 | validation_loss: 0.5427898168563843\nEpoch: 25 | train_loss: 0.5727422883889327 | validation_loss: 0.5377814173698425\nEpoch: 26 | train_loss: 0.5676013409780959 | validation_loss: 1.2054998874664307\nEpoch: 27 | train_loss: 0.5592595341537768 | validation_loss: 0.13950470089912415\nEpoch: 28 | train_loss: 0.5262662908062339 | validation_loss: 0.34643861651420593\nEpoch: 29 | train_loss: 0.5520231557699541 | validation_loss: 0.19155284762382507\nEpoch: 30 | train_loss: 0.5476370866720875 | validation_loss: 0.08122134208679199\nEpoch: 31 | train_loss: 0.539745842727522 | validation_loss: 0.19799964129924774\nEpoch: 32 | train_loss: 0.5426249022906026 | validation_loss: 0.3513737618923187\nEpoch: 33 | train_loss: 0.5397253624784449 | validation_loss: 0.4600915312767029\nEpoch: 34 | train_loss: 0.524396860661606 | validation_loss: 0.11414302885532379\nEpoch: 35 | train_loss: 0.5340759886118273 | validation_loss: 0.5274730920791626\nEpoch: 36 | train_loss: 0.5625634347088635 | validation_loss: 0.3969452679157257\nEpoch: 37 | train_loss: 0.5619061352529874 | validation_loss: 0.14751902222633362\nEpoch: 38 | train_loss: 0.5265185658354312 | validation_loss: 0.35307058691978455\nEpoch: 39 | train_loss: 0.5645509449144205 | validation_loss: 0.11530391126871109\nEpoch: 40 | train_loss: 0.5453658768286308 | validation_loss: 0.09477187693119049\nEpoch: 41 | train_loss: 0.571397924873357 | validation_loss: 0.39871981739997864\nEpoch: 42 | train_loss: 0.5811931053952625 | validation_loss: 0.9464401006698608\nEpoch: 43 | train_loss: 0.5739567352769276 | validation_loss: 0.3237219750881195\nEpoch: 44 | train_loss: 0.5542442722556492 | validation_loss: 0.32182440161705017\nEpoch: 45 | train_loss: 0.5600277989481887 | validation_loss: 0.6448650360107422\nEpoch: 46 | train_loss: 0.539244505421569 | validation_loss: 0.0895800068974495\nEpoch: 47 | train_loss: 0.5565683440460513 | validation_loss: 1.3262805938720703\nEpoch: 48 | train_loss: 0.5497646412501732 | validation_loss: 0.4305894374847412\nEpoch: 49 | train_loss: 0.5169288020115346 | validation_loss: 0.28135257959365845\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\n# state_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_8.pth')\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-20T16:08:03.473970Z","iopub.execute_input":"2023-04-20T16:08:03.474668Z","iopub.status.idle":"2023-04-20T16:08:03.532829Z","shell.execute_reply.started":"2023-04-20T16:08:03.474630Z","shell.execute_reply":"2023-04-20T16:08:03.531772Z"},"trusted":true},"execution_count":240,"outputs":[{"name":"stdout","text":"best epoch:  30\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'grp_norm_s.weight', 'grp_norm_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.5.linear1.weight', 'Transf1_t.encoder.layers.5.linear1.bias', 'Transf1_t.encoder.layers.5.linear2.weight', 'Transf1_t.encoder.layers.5.linear2.bias', 'Transf1_t.encoder.layers.5.norm1.weight', 'Transf1_t.encoder.layers.5.norm1.bias', 'Transf1_t.encoder.layers.5.norm2.weight', 'Transf1_t.encoder.layers.5.norm2.bias', 'Transf1_t.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.6.linear1.weight', 'Transf1_t.encoder.layers.6.linear1.bias', 'Transf1_t.encoder.layers.6.linear2.weight', 'Transf1_t.encoder.layers.6.linear2.bias', 'Transf1_t.encoder.layers.6.norm1.weight', 'Transf1_t.encoder.layers.6.norm1.bias', 'Transf1_t.encoder.layers.6.norm2.weight', 'Transf1_t.encoder.layers.6.norm2.bias', 'Transf1_t.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.7.linear1.weight', 'Transf1_t.encoder.layers.7.linear1.bias', 'Transf1_t.encoder.layers.7.linear2.weight', 'Transf1_t.encoder.layers.7.linear2.bias', 'Transf1_t.encoder.layers.7.norm1.weight', 'Transf1_t.encoder.layers.7.norm1.bias', 'Transf1_t.encoder.layers.7.norm2.weight', 'Transf1_t.encoder.layers.7.norm2.bias', 'Transf1_t.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.8.linear1.weight', 'Transf1_t.encoder.layers.8.linear1.bias', 'Transf1_t.encoder.layers.8.linear2.weight', 'Transf1_t.encoder.layers.8.linear2.bias', 'Transf1_t.encoder.layers.8.norm1.weight', 'Transf1_t.encoder.layers.8.norm1.bias', 'Transf1_t.encoder.layers.8.norm2.weight', 'Transf1_t.encoder.layers.8.norm2.bias', 'Transf1_t.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.9.linear1.weight', 'Transf1_t.encoder.layers.9.linear1.bias', 'Transf1_t.encoder.layers.9.linear2.weight', 'Transf1_t.encoder.layers.9.linear2.bias', 'Transf1_t.encoder.layers.9.norm1.weight', 'Transf1_t.encoder.layers.9.norm1.bias', 'Transf1_t.encoder.layers.9.norm2.weight', 'Transf1_t.encoder.layers.9.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":240,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-20T16:08:03.636876Z","iopub.execute_input":"2023-04-20T16:08:03.637188Z","iopub.status.idle":"2023-04-20T16:08:06.370190Z","shell.execute_reply.started":"2023-04-20T16:08:03.637158Z","shell.execute_reply":"2023-04-20T16:08:06.368821Z"},"trusted":true},"execution_count":241,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-20T16:08:06.372383Z","iopub.execute_input":"2023-04-20T16:08:06.373053Z","iopub.status.idle":"2023-04-20T16:08:06.381611Z","shell.execute_reply.started":"2023-04-20T16:08:06.373006Z","shell.execute_reply":"2023-04-20T16:08:06.380408Z"},"trusted":true},"execution_count":242,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 4, 1)\n[[7.9732066e-01]\n [2.0171529e-01]\n [4.4832341e-04]\n [5.1563873e-04]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:08:06.383757Z","iopub.execute_input":"2023-04-20T16:08:06.384524Z","iopub.status.idle":"2023-04-20T16:08:06.399888Z","shell.execute_reply.started":"2023-04-20T16:08:06.384407Z","shell.execute_reply":"2023-04-20T16:08:06.398339Z"},"trusted":true},"execution_count":243,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:08:06.404375Z","iopub.execute_input":"2023-04-20T16:08:06.411371Z","iopub.status.idle":"2023-04-20T16:08:06.421099Z","shell.execute_reply.started":"2023-04-20T16:08:06.411328Z","shell.execute_reply":"2023-04-20T16:08:06.419416Z"},"trusted":true},"execution_count":244,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-20T16:08:06.429247Z","iopub.execute_input":"2023-04-20T16:08:06.430042Z","iopub.status.idle":"2023-04-20T16:08:06.838773Z","shell.execute_reply.started":"2023-04-20T16:08:06.430002Z","shell.execute_reply":"2023-04-20T16:08:06.837784Z"},"trusted":true},"execution_count":245,"outputs":[{"name":"stdout","text":"accuracy:  0.75177304964539\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-20T15:54:18.869592Z","iopub.status.idle":"2023-04-20T15:54:18.870341Z","shell.execute_reply.started":"2023-04-20T15:54:18.870081Z","shell.execute_reply":"2023-04-20T15:54:18.870107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}