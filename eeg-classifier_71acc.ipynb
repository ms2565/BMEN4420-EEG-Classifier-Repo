{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-20T17:23:34.489093Z","iopub.execute_input":"2023-04-20T17:23:34.489840Z","iopub.status.idle":"2023-04-20T17:23:34.505344Z","shell.execute_reply.started":"2023-04-20T17:23:34.489797Z","shell.execute_reply":"2023-04-20T17:23:34.503208Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-20T20:51:26.857396Z","iopub.execute_input":"2023-04-20T20:51:26.858383Z","iopub.status.idle":"2023-04-20T20:51:26.866266Z","shell.execute_reply.started":"2023-04-20T20:51:26.858338Z","shell.execute_reply":"2023-04-20T20:51:26.865077Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# pip install oct2py\n#!apt-get install octave -y","metadata":{"execution":{"iopub.status.busy":"2023-04-20T20:51:27.754267Z","iopub.execute_input":"2023-04-20T20:51:27.754656Z","iopub.status.idle":"2023-04-20T20:51:27.759321Z","shell.execute_reply.started":"2023-04-20T20:51:27.754623Z","shell.execute_reply":"2023-04-20T20:51:27.758078Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T20:51:28.190379Z","iopub.execute_input":"2023-04-20T20:51:28.191456Z","iopub.status.idle":"2023-04-20T20:51:28.198290Z","shell.execute_reply.started":"2023-04-20T20:51:28.191416Z","shell.execute_reply":"2023-04-20T20:51:28.196598Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-20T20:51:29.046349Z","iopub.execute_input":"2023-04-20T20:51:29.046769Z","iopub.status.idle":"2023-04-20T20:51:29.171982Z","shell.execute_reply.started":"2023-04-20T20:51:29.046733Z","shell.execute_reply":"2023-04-20T20:51:29.170564Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T20:51:29.665257Z","iopub.execute_input":"2023-04-20T20:51:29.666365Z","iopub.status.idle":"2023-04-20T20:51:29.676982Z","shell.execute_reply.started":"2023-04-20T20:51:29.666317Z","shell.execute_reply":"2023-04-20T20:51:29.675733Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['EEGPT_Models', '__notebook_source__.ipynb', '.virtual_documents']"},"metadata":{}}]},{"cell_type":"code","source":"datatype = 'eeg'","metadata":{"execution":{"iopub.status.busy":"2023-04-20T20:51:30.125780Z","iopub.execute_input":"2023-04-20T20:51:30.127122Z","iopub.status.idle":"2023-04-20T20:51:30.132571Z","shell.execute_reply.started":"2023-04-20T20:51:30.127076Z","shell.execute_reply":"2023-04-20T20:51:30.131064Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if datatype == 'eeg':\n    sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n    # sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n    # data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\nelif datatype == 'ica':\n    sub01 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub01.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub02.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub03.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub04.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub05.mat')\n    sub06 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub06.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub07.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub08.mat')\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-20T20:51:30.554365Z","iopub.execute_input":"2023-04-20T20:51:30.555071Z","iopub.status.idle":"2023-04-20T20:51:36.024932Z","shell.execute_reply.started":"2023-04-20T20:51:30.555032Z","shell.execute_reply":"2023-04-20T20:51:36.023848Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-20T20:51:36.028020Z","iopub.execute_input":"2023-04-20T20:51:36.029069Z","iopub.status.idle":"2023-04-20T20:51:36.036890Z","shell.execute_reply.started":"2023-04-20T20:51:36.029026Z","shell.execute_reply":"2023-04-20T20:51:36.035407Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class testEEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(testEEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=17, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=10,stride=10)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=15,stride=1,padding=\"same\") # output should be \n    self.grp_norm_s = nn.GroupNorm(eeg_channels//6,eeg_channels)\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=(time_len//10),max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=time_len//10,outSize=4,numLayers=10,hiddenSize=2,numHeads=6,dropout=0.01)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=False, padding=\"same\")\n#     self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1))    \n    self.grp_norm_t = nn.GroupNorm(eeg_channels//6, eeg_channels)\n    # TRANSFORMER MODULE\n#     self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels//2,max_length=1500)\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=eeg_channels,outSize=4,numLayers=10,hiddenSize=2,numHeads=6,dropout=0.01)\n#     self.Transf1_t = EncoderTransformer(inSize=eeg_channels//2,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n    # Build Fully Connected Path\n    self.fc1 = nn.Linear(1260,1)\n\n  def forward(self, x):\n    # Spatial Pass\n    x_s = self.Conv1_s(x)\n#     print('x_s conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x_s avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x_s conv2: ',x_s.shape)\n    x_s = self.grp_norm_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s tf1: ',x_s.shape)\n    \n    # Temporal Pass\n    x_t = self.dwconv1_t(x)\n#     print('x_t conv1: ',x_t.shape)\n#     x_t = self.AvgPool1_t(x_t)\n    x_t = self.grp_norm_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    x_t = x_t.permute(0,2,1) # transpose to present time wise vectors to transformer encoder    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t tf1: ',x_t.shape)\n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n#     print('x_cat: ',x_cat.shape)\n    # Output Pass: Fully Connected into Softmax\n    x = self.fc1(x_cat)\n    x = torch.log_softmax(x,dim=1)\n    return x\n\nclass EEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(EEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n    self.AvgPool2_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n    self.AvgPool3_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=100,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=100,outSize=5,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=True, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1)) \n    self.conv2_t = nn.Conv1d(in_channels=eeg_channels//2,out_channels=eeg_channels//2, kernel_size=3, stride=1, bias = False, padding='same')\n    self.AvgPool2_t = nn.AvgPool2d(kernel_size=(2,1)) \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=60,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=60,outSize=5,numLayers=5,hiddenSize=5,numHeads=10,dropout=0.001)\n    # Build Fully Connected Path\n    if datatype == 'eeg':\n        self.fc1 = nn.Linear(1260,1)\n    elif datatype == 'ica':\n        self.fc1 = nn.Linear(1220,1)\n        \n\n  def forward(self, x):\n    # Spatial Pass\n    \n    x = x.to(torch.float32)\n#     print('x: ',x.shape)\n    x_s = self.Conv1_s(x)\n#     print('x conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x conv2: ',x_s.shape)\n#     x_s = self.AvgPool2_s(x_s)\n#     print('x avg2: ',x_s.shape)\n#     x_s = self.Conv3_s(x_s)\n#     print('x conv3: ',x_s.shape)\n#     x_s = self.AvgPool3_s(x_s)\n#     x_s = self.Conv4_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s_transf: ', x_s.shape)\n    \n    # Temporal Pass\n    #x_t = self.dwconv1_t(x)\n    #print('x_t conv1: ',x_t.shape)\n    #x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    #x_t = self.conv2_t(x_t)\n#     print('x_t conv2: ',x_t.shape)\n    #x_t = self.AvgPool2_t(x_t)\n#     print('x_t avg2: ',x_t.shape)\n    x_t = x.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n#     print('x_t avg1_permute: ',x_t.shape)    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t_transf: ', x_t.shape)\n    \n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n#     print('x_t transf1_perm: ',x_t.shape)\n#     print('x_s transf1_perm: ',x_s.shape)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n    # Output Pass: Fully Connected into Softmax\n#     print('x cat: ',x_cat.shape)\n    x = self.fc1(x_cat)\n#     print('x fc1: ',x.shape)\n    x = torch.log_softmax(x,dim=1)\n#     print('x softmax: ',x.shape)\n    return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-20T21:04:18.745549Z","iopub.execute_input":"2023-04-20T21:04:18.745935Z","iopub.status.idle":"2023-04-20T21:04:18.779903Z","shell.execute_reply.started":"2023-04-20T21:04:18.745902Z","shell.execute_reply":"2023-04-20T21:04:18.778673Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))\n\ndef mas2565_ICA(tensor):\n    pass\n    #return ICA_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-20T21:04:18.899346Z","iopub.execute_input":"2023-04-20T21:04:18.899722Z","iopub.status.idle":"2023-04-20T21:04:18.909367Z","shell.execute_reply.started":"2023-04-20T21:04:18.899688Z","shell.execute_reply":"2023-04-20T21:04:18.908322Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#print(data['sub01']['X_EEG_TRAIN'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T21:04:19.073526Z","iopub.execute_input":"2023-04-20T21:04:19.073845Z","iopub.status.idle":"2023-04-20T21:04:19.079680Z","shell.execute_reply.started":"2023-04-20T21:04:19.073815Z","shell.execute_reply":"2023-04-20T21:04:19.077564Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n    if i != 5:\n        subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n        subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n    for j in range(np.size(subSetY,0)):   \n        #print(np.shape(subSetX)[])\n        subx = subSetX[:,:,j]\n\n        #subx = mas2565_ICA(subx)\n        subx = mas2565_normalize(subx)\n        subx = mas2565_filter(subx)\n\n        #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n        #subx = noise(subx)\n        subx = torch.Tensor(subx)\n        #subx = mas2565_filter(subx)\n        #print(np.shape(subx))\n        suby = subSetY[j,:]\n        # miniSet = EEGData(subx,suby)\n        # print(np.shape(miniSet.y))\n        X.append(subx)\n        Y.append(suby)\n\n\n        # DEBUGGING PRINTS\n        #print(np.size(subSetY,0))\n        #print(np.shape(subSetX))\n        #print(np.shape(subSetY))\n        #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=10,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=10,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-20T21:04:19.218486Z","iopub.execute_input":"2023-04-20T21:04:19.219743Z","iopub.status.idle":"2023-04-20T21:04:20.823856Z","shell.execute_reply.started":"2023-04-20T21:04:19.219702Z","shell.execute_reply":"2023-04-20T21:04:20.822664Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = testEEGPT(eeg_channels=60, time_len=1200)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.0001)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-20T21:04:20.826315Z","iopub.execute_input":"2023-04-20T21:04:20.826836Z","iopub.status.idle":"2023-04-20T21:04:20.872436Z","shell.execute_reply.started":"2023-04-20T21:04:20.826792Z","shell.execute_reply":"2023-04-20T21:04:20.871510Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-20T21:04:20.874257Z","iopub.execute_input":"2023-04-20T21:04:20.875787Z","iopub.status.idle":"2023-04-20T21:04:20.884313Z","shell.execute_reply.started":"2023-04-20T21:04:20.875747Z","shell.execute_reply":"2023-04-20T21:04:20.883217Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"number of model params:  938933\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\nEPOCHS = 50\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode\n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-20T21:04:20.888865Z","iopub.execute_input":"2023-04-20T21:04:20.890054Z","iopub.status.idle":"2023-04-20T21:07:22.026112Z","shell.execute_reply.started":"2023-04-20T21:04:20.890012Z","shell.execute_reply":"2023-04-20T21:07:22.024930Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 1.1875757348948512 | validation_loss: 0.8154364705085755\nEpoch: 1 | train_loss: 0.7368540044488578 | validation_loss: 0.706096351146698\nEpoch: 2 | train_loss: 0.7042798112178671 | validation_loss: 0.7705471595128377\nEpoch: 3 | train_loss: 0.7037249392476576 | validation_loss: 0.713869305451711\nEpoch: 4 | train_loss: 0.7195208011002376 | validation_loss: 0.9560091972351075\nEpoch: 5 | train_loss: 0.6959185148107594 | validation_loss: 0.7341473778088887\nEpoch: 6 | train_loss: 0.6802670667911398 | validation_loss: 0.5947237292925517\nEpoch: 7 | train_loss: 0.6861054486241834 | validation_loss: 0.6603331565856934\nEpoch: 8 | train_loss: 0.6138993255023298 | validation_loss: 0.5618193864822387\nEpoch: 9 | train_loss: 0.5887142234835131 | validation_loss: 0.6940022110939026\nEpoch: 10 | train_loss: 0.6542164518915373 | validation_loss: 0.6616020798683167\nEpoch: 11 | train_loss: 0.5888662204660219 | validation_loss: 0.4463361918926239\nEpoch: 12 | train_loss: 0.5934208610962177 | validation_loss: 0.4759920537471771\nEpoch: 13 | train_loss: 0.5374607433532846 | validation_loss: 0.7916249632835388\nEpoch: 14 | train_loss: 0.5016114773421452 | validation_loss: 0.17193604111671448\nEpoch: 15 | train_loss: 0.5183982849121094 | validation_loss: 0.38083186745643616\nEpoch: 16 | train_loss: 0.5006151122265848 | validation_loss: 0.4617150604724884\nEpoch: 17 | train_loss: 0.5087706549414273 | validation_loss: 0.806143045425415\nEpoch: 18 | train_loss: 0.44979905105870344 | validation_loss: 0.41457327008247374\nEpoch: 19 | train_loss: 0.42852480966469336 | validation_loss: 0.7517304420471191\nEpoch: 20 | train_loss: 0.4749682931036785 | validation_loss: 0.5151450594266256\nEpoch: 21 | train_loss: 0.4351993002768221 | validation_loss: 1.1345087130864462\nEpoch: 22 | train_loss: 0.44396392178946525 | validation_loss: 0.5443164706230164\nEpoch: 23 | train_loss: 0.4051681587408329 | validation_loss: 0.21374422510464985\nEpoch: 24 | train_loss: 0.3600697034391864 | validation_loss: 0.22307136555512747\nEpoch: 25 | train_loss: 0.3871058467133292 | validation_loss: 0.28711800575256347\nEpoch: 26 | train_loss: 0.33437398718348865 | validation_loss: 0.20217791299025217\nEpoch: 27 | train_loss: 0.3562849559660616 | validation_loss: 0.3103127161661784\nEpoch: 28 | train_loss: 0.34873896580317926 | validation_loss: 0.8868639032046001\nEpoch: 29 | train_loss: 0.33738033935941497 | validation_loss: 0.2670283794403076\nEpoch: 30 | train_loss: 0.3158872685041921 | validation_loss: 0.17631095945835112\nEpoch: 31 | train_loss: 0.32796093436150714 | validation_loss: 0.1848747730255127\nEpoch: 32 | train_loss: 0.33527320160948 | validation_loss: 0.2646662950515747\nEpoch: 33 | train_loss: 0.45722256035640324 | validation_loss: 0.40986252427101133\nEpoch: 34 | train_loss: 0.4830860315725721 | validation_loss: 0.5605953335762024\nEpoch: 35 | train_loss: 0.3926866465601428 | validation_loss: 0.4291695396105448\nEpoch: 36 | train_loss: 0.3469128814236871 | validation_loss: 0.49956306417783103\nEpoch: 37 | train_loss: 0.33477221506422966 | validation_loss: 0.3724706172943115\nEpoch: 38 | train_loss: 0.3158926285546401 | validation_loss: 0.3109823644161224\nEpoch: 39 | train_loss: 0.4025605737135328 | validation_loss: 0.21853673656781514\nEpoch: 40 | train_loss: 0.3551266208804887 | validation_loss: 0.3487889607747396\nEpoch: 41 | train_loss: 0.31370984223382226 | validation_loss: 0.07130775898694992\nEpoch: 42 | train_loss: 0.29227297868708085 | validation_loss: 0.19575373828411102\nEpoch: 43 | train_loss: 0.31152121427244156 | validation_loss: 0.17734958231449127\nEpoch: 44 | train_loss: 0.26909871219561016 | validation_loss: 0.32460842132568357\nEpoch: 45 | train_loss: 0.3394167590244063 | validation_loss: 0.5285027345021566\nEpoch: 46 | train_loss: 0.36966420401786937 | validation_loss: 1.0152636766433716\nEpoch: 47 | train_loss: 0.3269292866361552 | validation_loss: 0.590625278155009\nEpoch: 48 | train_loss: 0.23803719312980257 | validation_loss: 0.09200328588485718\nEpoch: 49 | train_loss: 0.238914523530623 | validation_loss: 0.925406261285146\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\n# state_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_28.pth')\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-20T21:07:22.027672Z","iopub.execute_input":"2023-04-20T21:07:22.028724Z","iopub.status.idle":"2023-04-20T21:07:22.083463Z","shell.execute_reply.started":"2023-04-20T21:07:22.028679Z","shell.execute_reply":"2023-04-20T21:07:22.082330Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"best epoch:  41\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'grp_norm_s.weight', 'grp_norm_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'grp_norm_t.weight', 'grp_norm_t.bias', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.5.linear1.weight', 'Transf1_t.encoder.layers.5.linear1.bias', 'Transf1_t.encoder.layers.5.linear2.weight', 'Transf1_t.encoder.layers.5.linear2.bias', 'Transf1_t.encoder.layers.5.norm1.weight', 'Transf1_t.encoder.layers.5.norm1.bias', 'Transf1_t.encoder.layers.5.norm2.weight', 'Transf1_t.encoder.layers.5.norm2.bias', 'Transf1_t.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.6.linear1.weight', 'Transf1_t.encoder.layers.6.linear1.bias', 'Transf1_t.encoder.layers.6.linear2.weight', 'Transf1_t.encoder.layers.6.linear2.bias', 'Transf1_t.encoder.layers.6.norm1.weight', 'Transf1_t.encoder.layers.6.norm1.bias', 'Transf1_t.encoder.layers.6.norm2.weight', 'Transf1_t.encoder.layers.6.norm2.bias', 'Transf1_t.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.7.linear1.weight', 'Transf1_t.encoder.layers.7.linear1.bias', 'Transf1_t.encoder.layers.7.linear2.weight', 'Transf1_t.encoder.layers.7.linear2.bias', 'Transf1_t.encoder.layers.7.norm1.weight', 'Transf1_t.encoder.layers.7.norm1.bias', 'Transf1_t.encoder.layers.7.norm2.weight', 'Transf1_t.encoder.layers.7.norm2.bias', 'Transf1_t.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.8.linear1.weight', 'Transf1_t.encoder.layers.8.linear1.bias', 'Transf1_t.encoder.layers.8.linear2.weight', 'Transf1_t.encoder.layers.8.linear2.bias', 'Transf1_t.encoder.layers.8.norm1.weight', 'Transf1_t.encoder.layers.8.norm1.bias', 'Transf1_t.encoder.layers.8.norm2.weight', 'Transf1_t.encoder.layers.8.norm2.bias', 'Transf1_t.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.9.linear1.weight', 'Transf1_t.encoder.layers.9.linear1.bias', 'Transf1_t.encoder.layers.9.linear2.weight', 'Transf1_t.encoder.layers.9.linear2.bias', 'Transf1_t.encoder.layers.9.norm1.weight', 'Transf1_t.encoder.layers.9.norm1.bias', 'Transf1_t.encoder.layers.9.norm2.weight', 'Transf1_t.encoder.layers.9.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-20T21:07:22.084995Z","iopub.execute_input":"2023-04-20T21:07:22.085685Z","iopub.status.idle":"2023-04-20T21:07:24.664075Z","shell.execute_reply.started":"2023-04-20T21:07:22.085645Z","shell.execute_reply":"2023-04-20T21:07:24.662776Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-20T21:07:24.665717Z","iopub.execute_input":"2023-04-20T21:07:24.666104Z","iopub.status.idle":"2023-04-20T21:07:24.673953Z","shell.execute_reply.started":"2023-04-20T21:07:24.666061Z","shell.execute_reply":"2023-04-20T21:07:24.672476Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 4, 1)\n[[0.4533143 ]\n [0.54349095]\n [0.00164914]\n [0.00154561]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T21:07:24.675871Z","iopub.execute_input":"2023-04-20T21:07:24.676666Z","iopub.status.idle":"2023-04-20T21:07:24.692250Z","shell.execute_reply.started":"2023-04-20T21:07:24.676623Z","shell.execute_reply":"2023-04-20T21:07:24.691177Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T21:07:24.694679Z","iopub.execute_input":"2023-04-20T21:07:24.695558Z","iopub.status.idle":"2023-04-20T21:07:24.705346Z","shell.execute_reply.started":"2023-04-20T21:07:24.695513Z","shell.execute_reply":"2023-04-20T21:07:24.704145Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-20T21:07:24.708907Z","iopub.execute_input":"2023-04-20T21:07:24.709360Z","iopub.status.idle":"2023-04-20T21:07:25.003188Z","shell.execute_reply.started":"2023-04-20T21:07:24.709322Z","shell.execute_reply":"2023-04-20T21:07:25.002206Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"accuracy:  0.7163120567375887\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-20T21:07:25.004988Z","iopub.execute_input":"2023-04-20T21:07:25.005345Z","iopub.status.idle":"2023-04-20T21:07:25.043289Z","shell.execute_reply.started":"2023-04-20T21:07:25.005307Z","shell.execute_reply":"2023-04-20T21:07:25.042126Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}