{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY3z4fGrPY0j",
        "outputId": "d8cd90c1-f323-4d96-fd2f-3022af0a82e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/BMEN4420-EEG-Classifier-Repo'"
      ],
      "metadata": {
        "id": "wFSwK5USdu7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "yhOLV8UPTrKb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "import random\n",
        "import scipy.io as scio\n",
        "from torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#from Models.Transformer import TransformerModel\n",
        "#from Models.PositionalEncoding import LearnedPositionalEncoding\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK GPU RESOURCES\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"GPU available:\", cuda)\n",
        "\n",
        "torch.manual_seed(4460)# you don't have to set random seed beyond this block\n",
        "np.random.seed(4460)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue7yaBP0kCW-",
        "outputId": "a6e5aec6-bdd2-452f-9ed8-a1bde9ac282e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub01 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_1.mat')\n",
        "sub02 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_2.mat')\n",
        "sub03 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_3.mat')\n",
        "sub04 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_4.mat')\n",
        "sub05 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_5.mat')\n",
        "sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n",
        "sub07 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_7.mat')\n",
        "sub08 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_8.mat')\n",
        "data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n"
      ],
      "metadata": {
        "id": "lUT0FtKqgNPP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGData():\n",
        "  def __init__(self, sample, label):\n",
        "    self.x = sample\n",
        "    self.y = label\n",
        "    self.indices = list(range(np.size(self.y,0)))\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[self.indices[index]], self.y[self.indices[index]]\n",
        "  def shuffle(self):\n",
        "    random.shuffle(self.indices)\n",
        "  def __len__(self):\n",
        "    return (np.size(self.y,0))"
      ],
      "metadata": {
        "id": "CvUVk_oEw4CR"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGPT(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      eeg_channels = 60,\n",
        "      time_len = 1200\n",
        "               ):\n",
        "    super(EEGPT,self).__init__()\n",
        "    # BUILD SPATIAL PATH\n",
        "    ## CNN MODULE\n",
        "    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=17, stride=1, padding=\"same\")\n",
        "    self.AvgPool1_s = nn.AvgPool1d(kernel_size=32,stride=32)\n",
        "    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=15,stride=1,padding=\"valid\") # output should be \n",
        "    ## TRANSFORMER MODULE\n",
        "    self.PosEnc1_s = PositionalEncoder(embedding_dim=eeg_channels,max_length=1000)\n",
        "    self.Transf1_s = EncoderTransformer(inSize=eeg_channels,outSize=4,numLayers=3,hiddenSize=1,numHeads=6,dropout=0.01)\n",
        "\n",
        "    # BUILD TEMPORAL PATH\n",
        "    # CNN MODULE\n",
        "    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=False, padding=\"same\")\n",
        "    self.AvgPool1_t = nn.AvgPool2d(kernel_size=8)    \n",
        "    # TRANSFORMER MODULE\n",
        "    self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels,max_length=1000)\n",
        "    self.Transf1_t = EncoderTransformer(inSize=time_len,outSize=4,numLayers=3,hiddenSize=1,numHeads=6,dropout=0.01)\n",
        "    # Build Fully Connected Path\n",
        "    self.fc1 = nn.Linear(8,2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Spatial Pass\n",
        "    x_s = self.Conv1_s(x_s)\n",
        "    x_s = self.AvgPool1_s(x_s)\n",
        "    x_s = self.Conv2_s(x_s)\n",
        "    x_s = self.PosEnc1_s(x_s)\n",
        "    x_s = self.Transf1_s(x_s)\n",
        "    # Temporal Pass\n",
        "    x_t = self.dwconv1_t(x)\n",
        "    x_t = self.AvgPool1_t(x_t)\n",
        "    x_t = x_t.permute(0,2,1) # transpose to present time wise vectors to transformer encoder    \n",
        "    x_t = self.PosEnc1_t(x_t)\n",
        "    x_t = self.Transf1_t(x_t)\n",
        "    # Concatenation\n",
        "    x_cat = torch.cat(x_s, x_t)\n",
        "    # Output Pass: Fully Connected into Softmax\n",
        "    x = self.fc1(x_cat)\n",
        "    x = F.log_softmax(x)\n",
        "    return x\n",
        "\n",
        "class EncoderTransformer():\n",
        "  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n",
        "    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n",
        "    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n",
        "    self.fc1 = nn.Linear(outSize, outSize)\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self, embedding_dim, max_length=1000):\n",
        "    super(PositionalEncoder,self).__init__()\n",
        "    pe = torch.zeros(max_length, embedding_dim)\n",
        "    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, embedding_dim, 2).float()\n",
        "        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n",
        "    )\n",
        "    pe[:,0::2] = torch.sin(position * div_term)\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "    pe.unsqueeze(0).transpose(0,1)\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:x.size(0),:]\n",
        "\n"
      ],
      "metadata": {
        "id": "IjLUvymIhn45"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\n",
        "numSets = 8\n",
        "megaSet = []\n",
        "for i in range(numSets):\n",
        "  subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n",
        "  subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n",
        "  #print(np.size(subSetY,0))\n",
        "  for j in range(np.size(subSetY,0)):    \n",
        "    subx = subSetX[:,:,j]    \n",
        "    suby = subSetY[j,:]\n",
        "    miniSet = EEGData(subx,suby)\n",
        "    megaSet.append(miniSet)\n",
        "    \n",
        "    # DEBUGGING PRINTS\n",
        "    #print(np.size(subSetY,0))\n",
        "    #print(np.shape(subSetX))\n",
        "    #print(np.shape(subSetY))\n",
        "    #print(miniSet.__len__())\n",
        "\n",
        "MegaSet = ConcatDataset(megaSet)\n",
        "#MegaSet = RandomSampler(MegaSet)\n",
        "\n",
        "\n",
        "# Load Dataset using EEGData and Dataloader\n",
        "trainset, validset = random_split(MegaSet,[458, 115])\n",
        "trainloader = DataLoader(trainset,batch_size=3,shuffle=True)\n",
        "validloader = DataLoader(validset,batch_size=3,shuffle=True)"
      ],
      "metadata": {
        "id": "2tat7z1h7fPw"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "eegpt = EEGPT(eeg_channels=60, time_len=1200)\n",
        "\n",
        "# Call Optimizer\n",
        "adam = Adam(eegpt.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "u8WNB1li-GX0"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COUNT MODEL PARAMETERS\n",
        "param_count = 0;\n",
        "for param in eegpt.parameters():\n",
        "    param_count += param.numel()\n",
        "\n",
        "print('number of model params: ', param_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_dPdRf_hV-m",
        "outputId": "9f8c95f2-b731-4df7-d1e4-e3f200b997e9"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of model params:  118938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TRAINING\n",
        "EPOCHS = 25\n",
        "train_epoch_loss = list()\n",
        "validation_epoch_loss = list()\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = list()\n",
        "  valid_loss = list()\n",
        "  eegpt.train() # put model in train mode\n",
        "  for batch_index, (sample, label) in enumerate(trainloader):\n",
        "    if cuda:\n",
        "      train_pred = eegpt(sample.cuda())\n",
        "      # calculate loss\n",
        "      loss_fun = nn.CrossEntropyLoss()\n",
        "      loss = loss_fun(train_pred, label.cuda())\n",
        "      train_loss.append(loss.cpu().data.item())\n",
        "      # reset gradient\n",
        "      adam.zero_grad()\n",
        "      # back propagation\n",
        "      loss.backward()\n",
        "      # Update parameters\n",
        "      adam.step()\n",
        "train_epoch_loss.append(np.mean(train_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79_uGinHjAXm",
        "outputId": "cdbfba72-9aa8-4cbf-bee7-e359f20391d1"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TABS REFERENCE\n",
        "\n",
        "class up_conv_3D(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(up_conv_3D, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor = 2),\n",
        "            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.GroupNorm(8, ch_out),\n",
        "            # nn.BatchNorm3d(ch_out),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class conv_block_3D(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(conv_block_3D, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.GroupNorm(8, ch_out),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.GroupNorm(8, ch_out),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class resconv_block_3D(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(resconv_block_3D, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.GroupNorm(8, ch_out),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "            nn.GroupNorm(8, ch_out),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        residual = self.Conv_1x1(x)\n",
        "        x = self.conv(x)\n",
        "        return residual + x\n",
        "\n",
        "# Can add squeeze excitation layers if you want to try that as well.\n",
        "class ChannelSELayer3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D extension of Squeeze-and-Excitation (SE) block described in:\n",
        "        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n",
        "        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, reduction_ratio=8):\n",
        "        \"\"\"\n",
        "        :param num_channels: No of input channels\n",
        "        :param reduction_ratio: By how much should the num_channels should be reduced\n",
        "        \"\"\"\n",
        "        super(ChannelSELayer3D, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        num_channels_reduced = num_channels // reduction_ratio\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n",
        "        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"\n",
        "        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n",
        "        :return: output tensor\n",
        "        \"\"\"\n",
        "        batch_size, num_channels, D, H, W = input_tensor.size()\n",
        "        # Average along each channel\n",
        "        squeeze_tensor = self.avg_pool(input_tensor)\n",
        "\n",
        "        # channel excitation\n",
        "        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n",
        "        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n",
        "\n",
        "        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "class TABS(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_dim = 192,\n",
        "        patch_dim = 8,\n",
        "        img_ch = 1,\n",
        "        output_ch = 3,\n",
        "        embedding_dim = 512,\n",
        "        num_heads = 8,\n",
        "        num_layers = 4,\n",
        "        hidden_dim = 1728,\n",
        "        dropout_rate = 0.1,\n",
        "        attn_dropout_rate = 0.1,\n",
        "        ):\n",
        "        super(TABS,self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n",
        "\n",
        "        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n",
        "\n",
        "        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n",
        "\n",
        "        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n",
        "\n",
        "        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n",
        "\n",
        "        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n",
        "        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n",
        "\n",
        "        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n",
        "        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n",
        "\n",
        "        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n",
        "        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n",
        "\n",
        "        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n",
        "        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n",
        "        self.gn = nn.GroupNorm(8, 128)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.num_patches = int((img_dim // patch_dim) ** 3)\n",
        "        self.seq_length = self.num_patches\n",
        "        self.flatten_dim = 128 * img_ch\n",
        "\n",
        "        self.position_encoding = LearnedPositionalEncoding(\n",
        "            self.seq_length, embedding_dim, self.seq_length\n",
        "        )\n",
        "\n",
        "        self.act = nn.Softmax(dim=1)\n",
        "\n",
        "        self.reshaped_conv = conv_block_3D(512, 128)\n",
        "\n",
        "        self.transformer = TransformerModel(\n",
        "            embedding_dim,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            hidden_dim,\n",
        "\n",
        "            dropout_rate,\n",
        "            attn_dropout_rate,\n",
        "        )\n",
        "\n",
        "        self.conv_x = nn.Conv3d(\n",
        "            128,\n",
        "            embedding_dim,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "            )\n",
        "\n",
        "        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.img_dim = 192\n",
        "        self.patch_dim = 8\n",
        "        self.img_ch = 1\n",
        "        self.output_ch = 3\n",
        "        self.embedding_dim = 512\n",
        "\n",
        "    def forward(self,x):\n",
        "        # encoding path\n",
        "        x1 = self.Conv1(x)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.Conv2(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.Conv3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.Conv4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x = self.Conv5(x5)\n",
        "\n",
        "        x = self.gn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_x(x)\n",
        "\n",
        "        x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
        "        x = x.view(x.size(0), -1, self.embedding_dim)\n",
        "\n",
        "        x = self.position_encoding(x)\n",
        "\n",
        "        x, intmd_x = self.transformer(x)\n",
        "        x = self.pre_head_ln(x)\n",
        "\n",
        "        encoder_outputs = {}\n",
        "        all_keys = []\n",
        "        for i in [1, 2, 3, 4]:\n",
        "            val = str(2 * i - 1)\n",
        "            _key = 'Z' + str(i)\n",
        "            all_keys.append(_key)\n",
        "            encoder_outputs[_key] = intmd_x[val]\n",
        "        all_keys.reverse()\n",
        "\n",
        "        x = encoder_outputs[all_keys[0]]\n",
        "        x = self._reshape_output(x)\n",
        "        x = self.reshaped_conv(x)\n",
        "\n",
        "        d5 = self.Up5(x)\n",
        "        d5 = torch.cat((x4,d5),dim=1)\n",
        "        d5 = self.Up_conv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((x3,d4),dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((x2,d3),dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((x1,d2),dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "\n",
        "        d1 = self.act(d1)\n",
        "\n",
        "        return d1\n",
        "\n",
        "    def _reshape_output(self, x):\n",
        "        x = x.view(\n",
        "            x.size(0),\n",
        "            int(self.img_dim//2 / self.patch_dim),\n",
        "            int(self.img_dim//2 / self.patch_dim),\n",
        "            int(self.img_dim//2 / self.patch_dim),\n",
        "            self.embedding_dim,\n",
        "        )\n",
        "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "MLfq9obROrbO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}