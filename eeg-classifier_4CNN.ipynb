{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-11T03:59:19.369168Z","iopub.execute_input":"2023-04-11T03:59:19.369536Z","iopub.status.idle":"2023-04-11T03:59:19.374166Z","shell.execute_reply.started":"2023-04-11T03:59:19.369503Z","shell.execute_reply":"2023-04-11T03:59:19.372838Z"},"trusted":true},"execution_count":552,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-11T03:59:19.412872Z","iopub.execute_input":"2023-04-11T03:59:19.413209Z","iopub.status.idle":"2023-04-11T03:59:19.420295Z","shell.execute_reply.started":"2023-04-11T03:59:19.413172Z","shell.execute_reply":"2023-04-11T03:59:19.419224Z"},"trusted":true},"execution_count":553,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-11T03:59:19.445355Z","iopub.execute_input":"2023-04-11T03:59:19.445654Z","iopub.status.idle":"2023-04-11T03:59:19.451411Z","shell.execute_reply.started":"2023-04-11T03:59:19.445625Z","shell.execute_reply":"2023-04-11T03:59:19.449991Z"},"trusted":true},"execution_count":554,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-11T03:59:19.480271Z","iopub.execute_input":"2023-04-11T03:59:19.480566Z","iopub.status.idle":"2023-04-11T03:59:19.486735Z","shell.execute_reply.started":"2023-04-11T03:59:19.480538Z","shell.execute_reply":"2023-04-11T03:59:19.485543Z"},"trusted":true},"execution_count":555,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-11T03:59:19.524111Z","iopub.execute_input":"2023-04-11T03:59:19.524420Z","iopub.status.idle":"2023-04-11T03:59:19.533980Z","shell.execute_reply.started":"2023-04-11T03:59:19.524389Z","shell.execute_reply":"2023-04-11T03:59:19.532912Z"},"trusted":true},"execution_count":556,"outputs":[{"execution_count":556,"output_type":"execute_result","data":{"text/plain":"['.virtual_documents', 'EEGPT_Models', '__notebook_source__.ipynb']"},"metadata":{}}]},{"cell_type":"code","source":"sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\nsub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\nsub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\nsub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\nsub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n# sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\nsub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\nsub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n# data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\ndata = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\n","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-11T03:59:19.566274Z","iopub.execute_input":"2023-04-11T03:59:19.566588Z","iopub.status.idle":"2023-04-11T03:59:21.644766Z","shell.execute_reply.started":"2023-04-11T03:59:19.566557Z","shell.execute_reply":"2023-04-11T03:59:21.643705Z"},"trusted":true},"execution_count":557,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-11T03:59:21.647016Z","iopub.execute_input":"2023-04-11T03:59:21.647399Z","iopub.status.idle":"2023-04-11T03:59:21.656666Z","shell.execute_reply.started":"2023-04-11T03:59:21.647361Z","shell.execute_reply":"2023-04-11T03:59:21.655684Z"},"trusted":true},"execution_count":558,"outputs":[]},{"cell_type":"code","source":"class EEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(EEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n    self.AvgPool2_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n    self.AvgPool3_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=10,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=10,outSize=2,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=False, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(6,1))    \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=10,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=10,outSize=2,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n    # Build Fully Connected Path\n    self.fc1 = nn.Linear(1260,1)\n\n  def forward(self, x):\n    # Spatial Pass\n    \n    x = x.to(torch.float32)\n    #print('x: ',x.shape)\n    x_s = self.Conv1_s(x)\n    #print('x conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n    #print('x avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n    #print('x conv2: ',x_s.shape)\n    x_s = self.AvgPool2_s(x_s)\n    #print('x avg2: ',x_s.shape)\n    x_s = self.Conv3_s(x_s)\n    #print('x conv3: ',x_s.shape)\n    x_s = self.AvgPool3_s(x_s)\n    x_s = self.Conv4_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n    \n    # Temporal Pass\n    x_t = self.dwconv1_t(x)\n    #print('x_t conv1: ',x_t.shape)\n    x_t = self.AvgPool1_t(x_t)\n    #print('x_t avg1: ',x_t.shape)\n    x_t = x_t.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n    #print('x_t avg1_permute: ',x_t.shape)    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n    \n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n    #print('x_t transf1: ',x_t.shape)\n    #print('x_s transf1: ',x_s.shape)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n    # Output Pass: Fully Connected into Softmax\n    #print('x cat: ',x_cat.shape)\n    x = self.fc1(x_cat)\n    #print('x fc1: ',x.shape)\n    x = torch.log_softmax(x,dim=1)\n    #print('x softmax: ',x.shape)\n    return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-11T03:59:21.658742Z","iopub.execute_input":"2023-04-11T03:59:21.659533Z","iopub.status.idle":"2023-04-11T03:59:21.681361Z","shell.execute_reply.started":"2023-04-11T03:59:21.659495Z","shell.execute_reply":"2023-04-11T03:59:21.680384Z"},"trusted":true},"execution_count":559,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))","metadata":{"execution":{"iopub.status.busy":"2023-04-11T03:59:21.685644Z","iopub.execute_input":"2023-04-11T03:59:21.686045Z","iopub.status.idle":"2023-04-11T03:59:21.697591Z","shell.execute_reply.started":"2023-04-11T03:59:21.686018Z","shell.execute_reply":"2023-04-11T03:59:21.696422Z"},"trusted":true},"execution_count":560,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n  if i != 5:\n    subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n    subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n  for j in range(np.size(subSetY,0)):   \n    #print(np.shape(subSetX)[])\n    subx = subSetX[:,:,j]\n    \n    subx = mas2565_normalize(subx)\n    subx = mas2565_filter(subx)\n    #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n    #subx = noise(subx)\n    subx = torch.Tensor(subx)\n    #subx = mas2565_filter(subx)\n    #print(np.shape(subx))\n    suby = subSetY[j,:]\n    # miniSet = EEGData(subx,suby)\n    # print(np.shape(miniSet.y))\n    X.append(subx)\n    Y.append(suby)\n    \n    \n    # DEBUGGING PRINTS\n    #print(np.size(subSetY,0))\n    #print(np.shape(subSetX))\n    #print(np.shape(subSetY))\n    #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=3,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=3,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-11T03:59:21.700744Z","iopub.execute_input":"2023-04-11T03:59:21.701686Z","iopub.status.idle":"2023-04-11T03:59:23.238515Z","shell.execute_reply.started":"2023-04-11T03:59:21.701648Z","shell.execute_reply":"2023-04-11T03:59:23.237483Z"},"trusted":true},"execution_count":561,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = EEGPT(eeg_channels=60, time_len=1200)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.0001)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-11T03:59:23.240201Z","iopub.execute_input":"2023-04-11T03:59:23.240564Z","iopub.status.idle":"2023-04-11T03:59:23.280177Z","shell.execute_reply.started":"2023-04-11T03:59:23.240522Z","shell.execute_reply":"2023-04-11T03:59:23.279259Z"},"trusted":true},"execution_count":562,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-11T03:59:23.281915Z","iopub.execute_input":"2023-04-11T03:59:23.282304Z","iopub.status.idle":"2023-04-11T03:59:23.288860Z","shell.execute_reply.started":"2023-04-11T03:59:23.282268Z","shell.execute_reply":"2023-04-11T03:59:23.287742Z"},"trusted":true},"execution_count":563,"outputs":[{"name":"stdout","text":"number of model params:  178945\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\nEPOCHS = 30\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode\n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-11T03:59:23.290528Z","iopub.execute_input":"2023-04-11T03:59:23.291335Z","iopub.status.idle":"2023-04-11T04:02:28.763081Z","shell.execute_reply.started":"2023-04-11T03:59:23.291290Z","shell.execute_reply":"2023-04-11T04:02:28.761996Z"},"trusted":true},"execution_count":564,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 0.699906799942255 | validation_loss: 0.6280850172042847\nEpoch: 1 | train_loss: 0.6902581006288528 | validation_loss: 0.6050384044647217\nEpoch: 2 | train_loss: 0.6578865538661679 | validation_loss: 0.9829349517822266\nEpoch: 3 | train_loss: 0.6254385377590855 | validation_loss: 0.3439868092536926\nEpoch: 4 | train_loss: 0.6331198035428921 | validation_loss: 0.31096360087394714\nEpoch: 5 | train_loss: 0.6023295049866041 | validation_loss: 0.3839764893054962\nEpoch: 6 | train_loss: 0.5873759010185798 | validation_loss: 1.3042490482330322\nEpoch: 7 | train_loss: 0.5471847681328654 | validation_loss: 0.4920532703399658\nEpoch: 8 | train_loss: 0.5287422205631932 | validation_loss: 0.1767033040523529\nEpoch: 9 | train_loss: 0.5195565073130032 | validation_loss: 0.2444569617509842\nEpoch: 10 | train_loss: 0.49791763971249264 | validation_loss: 0.22284328937530518\nEpoch: 11 | train_loss: 0.48359497675361734 | validation_loss: 0.23568078875541687\nEpoch: 12 | train_loss: 0.4558175226363043 | validation_loss: 0.5158576965332031\nEpoch: 13 | train_loss: 0.4486283167110135 | validation_loss: 0.141511932015419\nEpoch: 14 | train_loss: 0.43631147666989517 | validation_loss: 0.06681808829307556\nEpoch: 15 | train_loss: 0.4062959871565302 | validation_loss: 0.31007862091064453\nEpoch: 16 | train_loss: 0.4007068284166356 | validation_loss: 1.283672571182251\nEpoch: 17 | train_loss: 0.36641558112266165 | validation_loss: 0.07899615913629532\nEpoch: 18 | train_loss: 0.3617501985281706 | validation_loss: 0.027220234274864197\nEpoch: 19 | train_loss: 0.36510254062401754 | validation_loss: 0.8125290274620056\nEpoch: 20 | train_loss: 0.34409689422075945 | validation_loss: 0.47221484780311584\nEpoch: 21 | train_loss: 0.3320456993339273 | validation_loss: 0.5902979969978333\nEpoch: 22 | train_loss: 0.2891989098667788 | validation_loss: 0.02470642887055874\nEpoch: 23 | train_loss: 0.3140815571920636 | validation_loss: 0.03441636636853218\nEpoch: 24 | train_loss: 0.29434398730518296 | validation_loss: 0.7630442976951599\nEpoch: 25 | train_loss: 0.2665288261681174 | validation_loss: 0.2454567849636078\nEpoch: 26 | train_loss: 0.24781810094524795 | validation_loss: 0.03143323212862015\nEpoch: 27 | train_loss: 0.24162680876421896 | validation_loss: 1.4646750688552856\nEpoch: 28 | train_loss: 0.22580380534539776 | validation_loss: 0.27418485283851624\nEpoch: 29 | train_loss: 0.21362040669676693 | validation_loss: 0.13239383697509766\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-11T04:02:28.764772Z","iopub.execute_input":"2023-04-11T04:02:28.765155Z","iopub.status.idle":"2023-04-11T04:02:28.814430Z","shell.execute_reply.started":"2023-04-11T04:02:28.765115Z","shell.execute_reply":"2023-04-11T04:02:28.813425Z"},"trusted":true},"execution_count":565,"outputs":[{"name":"stdout","text":"best epoch:  22\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'Conv3_s.weight', 'Conv3_s.bias', 'Conv4_s.weight', 'Conv4_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.5.linear1.weight', 'Transf1_t.encoder.layers.5.linear1.bias', 'Transf1_t.encoder.layers.5.linear2.weight', 'Transf1_t.encoder.layers.5.linear2.bias', 'Transf1_t.encoder.layers.5.norm1.weight', 'Transf1_t.encoder.layers.5.norm1.bias', 'Transf1_t.encoder.layers.5.norm2.weight', 'Transf1_t.encoder.layers.5.norm2.bias', 'Transf1_t.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.6.linear1.weight', 'Transf1_t.encoder.layers.6.linear1.bias', 'Transf1_t.encoder.layers.6.linear2.weight', 'Transf1_t.encoder.layers.6.linear2.bias', 'Transf1_t.encoder.layers.6.norm1.weight', 'Transf1_t.encoder.layers.6.norm1.bias', 'Transf1_t.encoder.layers.6.norm2.weight', 'Transf1_t.encoder.layers.6.norm2.bias', 'Transf1_t.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.7.linear1.weight', 'Transf1_t.encoder.layers.7.linear1.bias', 'Transf1_t.encoder.layers.7.linear2.weight', 'Transf1_t.encoder.layers.7.linear2.bias', 'Transf1_t.encoder.layers.7.norm1.weight', 'Transf1_t.encoder.layers.7.norm1.bias', 'Transf1_t.encoder.layers.7.norm2.weight', 'Transf1_t.encoder.layers.7.norm2.bias', 'Transf1_t.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.8.linear1.weight', 'Transf1_t.encoder.layers.8.linear1.bias', 'Transf1_t.encoder.layers.8.linear2.weight', 'Transf1_t.encoder.layers.8.linear2.bias', 'Transf1_t.encoder.layers.8.norm1.weight', 'Transf1_t.encoder.layers.8.norm1.bias', 'Transf1_t.encoder.layers.8.norm2.weight', 'Transf1_t.encoder.layers.8.norm2.bias', 'Transf1_t.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.9.linear1.weight', 'Transf1_t.encoder.layers.9.linear1.bias', 'Transf1_t.encoder.layers.9.linear2.weight', 'Transf1_t.encoder.layers.9.linear2.bias', 'Transf1_t.encoder.layers.9.norm1.weight', 'Transf1_t.encoder.layers.9.norm1.bias', 'Transf1_t.encoder.layers.9.norm2.weight', 'Transf1_t.encoder.layers.9.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":565,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-11T04:02:28.817831Z","iopub.execute_input":"2023-04-11T04:02:28.818345Z","iopub.status.idle":"2023-04-11T04:02:30.789281Z","shell.execute_reply.started":"2023-04-11T04:02:28.818316Z","shell.execute_reply":"2023-04-11T04:02:30.788256Z"},"trusted":true},"execution_count":566,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-11T04:02:30.790940Z","iopub.execute_input":"2023-04-11T04:02:30.791309Z","iopub.status.idle":"2023-04-11T04:02:30.799853Z","shell.execute_reply.started":"2023-04-11T04:02:30.791269Z","shell.execute_reply":"2023-04-11T04:02:30.796876Z"},"trusted":true},"execution_count":567,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 2, 1)\n[[0.0407677]\n [0.9592323]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:02:30.801880Z","iopub.execute_input":"2023-04-11T04:02:30.802837Z","iopub.status.idle":"2023-04-11T04:02:30.815083Z","shell.execute_reply.started":"2023-04-11T04:02:30.802790Z","shell.execute_reply":"2023-04-11T04:02:30.813715Z"},"trusted":true},"execution_count":568,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-11T04:02:30.816819Z","iopub.execute_input":"2023-04-11T04:02:30.817543Z","iopub.status.idle":"2023-04-11T04:02:30.826753Z","shell.execute_reply.started":"2023-04-11T04:02:30.817490Z","shell.execute_reply":"2023-04-11T04:02:30.825428Z"},"trusted":true},"execution_count":569,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-11T04:02:30.828583Z","iopub.execute_input":"2023-04-11T04:02:30.829019Z","iopub.status.idle":"2023-04-11T04:02:31.122797Z","shell.execute_reply.started":"2023-04-11T04:02:30.828982Z","shell.execute_reply":"2023-04-11T04:02:31.121721Z"},"trusted":true},"execution_count":570,"outputs":[{"name":"stdout","text":"accuracy:  0.7092198581560284\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-11T04:02:31.125568Z","iopub.execute_input":"2023-04-11T04:02:31.125932Z","iopub.status.idle":"2023-04-11T04:02:31.159786Z","shell.execute_reply.started":"2023-04-11T04:02:31.125886Z","shell.execute_reply":"2023-04-11T04:02:31.158797Z"},"trusted":true},"execution_count":571,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}