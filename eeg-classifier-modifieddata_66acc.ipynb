{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-21T02:59:34.588322Z","iopub.execute_input":"2023-04-21T02:59:34.588693Z","iopub.status.idle":"2023-04-21T02:59:34.623938Z","shell.execute_reply.started":"2023-04-21T02:59:34.588659Z","shell.execute_reply":"2023-04-21T02:59:34.622835Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\nimport torch.optim.lr_scheduler as lr_scheduler\n\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-21T02:59:34.626255Z","iopub.execute_input":"2023-04-21T02:59:34.626697Z","iopub.status.idle":"2023-04-21T02:59:39.673276Z","shell.execute_reply.started":"2023-04-21T02:59:34.626653Z","shell.execute_reply":"2023-04-21T02:59:39.671872Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# pip install oct2py\n#!apt-get install octave -y","metadata":{"execution":{"iopub.status.busy":"2023-04-21T02:59:39.675852Z","iopub.execute_input":"2023-04-21T02:59:39.676479Z","iopub.status.idle":"2023-04-21T02:59:39.681368Z","shell.execute_reply.started":"2023-04-21T02:59:39.676437Z","shell.execute_reply":"2023-04-21T02:59:39.680190Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-21T02:59:39.683538Z","iopub.execute_input":"2023-04-21T02:59:39.684055Z","iopub.status.idle":"2023-04-21T02:59:39.694523Z","shell.execute_reply.started":"2023-04-21T02:59:39.684013Z","shell.execute_reply":"2023-04-21T02:59:39.692943Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-21T02:59:39.698736Z","iopub.execute_input":"2023-04-21T02:59:39.699157Z","iopub.status.idle":"2023-04-21T02:59:39.837459Z","shell.execute_reply.started":"2023-04-21T02:59:39.699106Z","shell.execute_reply":"2023-04-21T02:59:39.836215Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T02:59:39.840095Z","iopub.execute_input":"2023-04-21T02:59:39.841286Z","iopub.status.idle":"2023-04-21T02:59:39.855602Z","shell.execute_reply.started":"2023-04-21T02:59:39.841240Z","shell.execute_reply":"2023-04-21T02:59:39.854158Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['.virtual_documents', '__notebook_source__.ipynb', 'EEGPT_Models']"},"metadata":{}}]},{"cell_type":"code","source":"datatype = 'modeeg'","metadata":{"execution":{"iopub.status.busy":"2023-04-21T02:59:39.859631Z","iopub.execute_input":"2023-04-21T02:59:39.860097Z","iopub.status.idle":"2023-04-21T02:59:39.866670Z","shell.execute_reply.started":"2023-04-21T02:59:39.860050Z","shell.execute_reply":"2023-04-21T02:59:39.865033Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if datatype == 'eeg':\n    sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n    # sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n    # data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\nelif datatype == 'modeeg':\n    sub01 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub01.mat')\n    sub02 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub02.mat')\n    sub03 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub03.mat')\n    sub04 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub04.mat')\n    sub05 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub05.mat')\n    sub06 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub06.mat')\n    sub07 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub07.mat')\n    sub08 = scio.loadmat('/kaggle/input/mod-eeg-tensors-imputed/Modified_EEG_Tensors/sub08.mat')\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-21T02:59:39.868913Z","iopub.execute_input":"2023-04-21T02:59:39.871870Z","iopub.status.idle":"2023-04-21T02:59:41.427989Z","shell.execute_reply.started":"2023-04-21T02:59:39.871826Z","shell.execute_reply":"2023-04-21T02:59:41.426888Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-21T02:59:41.429952Z","iopub.execute_input":"2023-04-21T02:59:41.430394Z","iopub.status.idle":"2023-04-21T02:59:41.439398Z","shell.execute_reply.started":"2023-04-21T02:59:41.430351Z","shell.execute_reply":"2023-04-21T02:59:41.437029Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class testEEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 182\n               ):\n    super(testEEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=17, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=2,stride=2)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=14,stride=1,padding=6) # output should be \n    self.grp_norm_s = nn.GroupNorm(eeg_channels//6,eeg_channels)\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=(90),max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=(90),outSize=4,numLayers=10,hiddenSize=1,numHeads=10,dropout=0.01)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1))    \n#     self.grp_norm_t = nn.GroupNorm(eeg_channels//6, eeg_channels)\n    # TRANSFORMER MODULE\n#     self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels//2,max_length=1500)\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels//2,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=eeg_channels//2,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n#     self.Transf1_t = EncoderTransformer(inSize=eeg_channels//2,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n    # Build Fully Connected Path\n    self.fc1 = nn.Linear(60+182,1)\n\n  def forward(self, x):\n    # Spatial Pass\n    x_s = self.Conv1_s(x)\n#     print('x_s conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x_s avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x_s conv2: ',x_s.shape)\n#     x_s = self.grp_norm_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s tf1: ',x_s.shape)\n    \n    # Temporal Pass\n    x_t = self.dwconv1_t(x)\n#     print('x_t conv1: ',x_t.shape)\n    x_t = self.AvgPool1_t(x_t)\n#     x_t = self.grp_norm_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    x_t = x_t.permute(0,2,1) # transpose to present time wise vectors to transformer encoder    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t tf1: ',x_t.shape)\n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n#     print('x_cat: ',x_cat.shape)\n    # Output Pass: Fully Connected into Softmax\n    x = self.fc1(x_cat)\n    x = torch.log_softmax(x,dim=1)\n    return x\n\n# class EEGPT(nn.Module):\n#   def __init__(\n#       self,\n#       eeg_channels = 60,\n#       time_len = 1200\n#                ):\n#     super(EEGPT,self).__init__()\n#     # BUILD SPATIAL PATH\n#     ## CNN MODULE\n#     self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n#     self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n#     self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n#     self.AvgPool2_s = nn.AvgPool1d(kernel_size=3,stride=3)\n#     self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n#     self.AvgPool3_s = nn.AvgPool1d(kernel_size=3,stride=3)\n#     self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n#     ## TRANSFORMER MODULE\n#     self.PosEnc1_s = PositionalEncoder(embedding_dim=100,max_length=1000)\n#     self.Transf1_s = EncoderTransformer(inSize=100,outSize=5,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n#     # BUILD TEMPORAL PATH\n#     # CNN MODULE\n#     self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=True, padding=\"same\")\n#     self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1)) \n#     self.conv2_t = nn.Conv1d(in_channels=eeg_channels//2,out_channels=eeg_channels//2, kernel_size=3, stride=1, bias = False, padding='same')\n#     self.AvgPool2_t = nn.AvgPool2d(kernel_size=(2,1)) \n#     # TRANSFORMER MODULE\n#     self.PosEnc1_t = PositionalEncoder(embedding_dim=60,max_length=1500)\n#     self.Transf1_t = EncoderTransformer(inSize=60,outSize=5,numLayers=5,hiddenSize=5,numHeads=10,dropout=0.001)\n#     # Build Fully Connected Path\n#     if datatype == 'eeg':\n#         self.fc1 = nn.Linear(1260,1)\n#     elif datatype == 'ica':\n#         self.fc1 = nn.Linear(1220,1)\n        \n\n#   def forward(self, x):\n#     # Spatial Pass\n    \n#     x = x.to(torch.float32)\n# #     print('x: ',x.shape)\n#     x_s = self.Conv1_s(x)\n# #     print('x conv1: ',x_s.shape)\n#     x_s = self.AvgPool1_s(x_s)\n# #     print('x avg1: ',x_s.shape)\n#     x_s = self.Conv2_s(x_s)\n# #     print('x conv2: ',x_s.shape)\n# #     x_s = self.AvgPool2_s(x_s)\n# #     print('x avg2: ',x_s.shape)\n# #     x_s = self.Conv3_s(x_s)\n# #     print('x conv3: ',x_s.shape)\n# #     x_s = self.AvgPool3_s(x_s)\n# #     x_s = self.Conv4_s(x_s)\n#     x_s = self.PosEnc1_s(x_s)\n#     x_s = self.Transf1_s(x_s)\n# #     print('x_s_transf: ', x_s.shape)\n    \n#     # Temporal Pass\n#     #x_t = self.dwconv1_t(x)\n#     #print('x_t conv1: ',x_t.shape)\n#     #x_t = self.AvgPool1_t(x_t)\n# #     print('x_t avg1: ',x_t.shape)\n#     #x_t = self.conv2_t(x_t)\n# #     print('x_t conv2: ',x_t.shape)\n#     #x_t = self.AvgPool2_t(x_t)\n# #     print('x_t avg2: ',x_t.shape)\n#     x_t = x.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n# #     print('x_t avg1_permute: ',x_t.shape)    \n#     x_t = self.PosEnc1_t(x_t)\n#     x_t = self.Transf1_t(x_t)\n# #     print('x_t_transf: ', x_t.shape)\n    \n#     # Concatenation\n#     x_s = x_s.permute(0,2,1)\n#     x_t = x_t.permute(0,2,1)\n# #     print('x_t transf1_perm: ',x_t.shape)\n# #     print('x_s transf1_perm: ',x_s.shape)\n#     x_cat = torch.cat((x_s, x_t),dim=2)\n#     # Output Pass: Fully Connected into Softmax\n# #     print('x cat: ',x_cat.shape)\n#     x = self.fc1(x_cat)\n# #     print('x fc1: ',x.shape)\n#     x = torch.log_softmax(x,dim=1)\n# #     print('x softmax: ',x.shape)\n#     return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-21T03:02:48.843767Z","iopub.execute_input":"2023-04-21T03:02:48.844643Z","iopub.status.idle":"2023-04-21T03:02:48.868348Z","shell.execute_reply.started":"2023-04-21T03:02:48.844591Z","shell.execute_reply":"2023-04-21T03:02:48.866918Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))\n\ndef mas2565_ICA(tensor):\n    pass\n    #return ICA_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-21T03:02:49.027881Z","iopub.execute_input":"2023-04-21T03:02:49.028400Z","iopub.status.idle":"2023-04-21T03:02:49.046746Z","shell.execute_reply.started":"2023-04-21T03:02:49.028356Z","shell.execute_reply":"2023-04-21T03:02:49.045559Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#print(data['sub01']['X_EEG_TRAIN'])","metadata":{"execution":{"iopub.status.busy":"2023-04-21T03:02:49.198458Z","iopub.execute_input":"2023-04-21T03:02:49.199482Z","iopub.status.idle":"2023-04-21T03:02:49.204943Z","shell.execute_reply.started":"2023-04-21T03:02:49.199423Z","shell.execute_reply":"2023-04-21T03:02:49.203542Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n    if i != 5:\n        subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n        subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n    for j in range(np.size(subSetY,0)):   \n        #print(np.shape(subSetX)[])\n        subx = subSetX[:,:,j]\n\n        #subx = mas2565_ICA(subx)\n        subx = mas2565_normalize(subx)\n        subx = mas2565_filter(subx)\n\n        #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n        #subx = noise(subx)\n        subx = torch.Tensor(subx)\n        #subx = mas2565_filter(subx)\n        #print(np.shape(subx))\n        suby = subSetY[j,:]\n        # miniSet = EEGData(subx,suby)\n        # print(np.shape(miniSet.y))\n        X.append(subx)\n        Y.append(suby)\n\n\n        # DEBUGGING PRINTS\n        #print(np.size(subSetY,0))\n        #print(np.shape(subSetX))\n        #print(np.shape(subSetY))\n        #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=10,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=10,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-21T03:02:49.378722Z","iopub.execute_input":"2023-04-21T03:02:49.379542Z","iopub.status.idle":"2023-04-21T03:02:50.368869Z","shell.execute_reply.started":"2023-04-21T03:02:49.379499Z","shell.execute_reply":"2023-04-21T03:02:50.367681Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = testEEGPT(eeg_channels=60, time_len=182)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.0001)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-21T03:02:50.371665Z","iopub.execute_input":"2023-04-21T03:02:50.372108Z","iopub.status.idle":"2023-04-21T03:02:50.415717Z","shell.execute_reply.started":"2023-04-21T03:02:50.372067Z","shell.execute_reply":"2023-04-21T03:02:50.414641Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-21T03:02:50.417625Z","iopub.execute_input":"2023-04-21T03:02:50.419706Z","iopub.status.idle":"2023-04-21T03:02:50.429453Z","shell.execute_reply.started":"2023-04-21T03:02:50.419663Z","shell.execute_reply":"2023-04-21T03:02:50.428331Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"number of model params:  526773\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\n# scheduler = lr_scheduler.CosineAnnealingLR(adam, T_max=40, eta_min=0.00005)\nEPOCHS = 40\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode  \n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      \n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n#       scheduler.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-21T03:02:50.431954Z","iopub.execute_input":"2023-04-21T03:02:50.432556Z","iopub.status.idle":"2023-04-21T03:04:24.559267Z","shell.execute_reply.started":"2023-04-21T03:02:50.432518Z","shell.execute_reply":"2023-04-21T03:04:24.557024Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 1.114232794991855 | validation_loss: 1.109923815727234\nEpoch: 1 | train_loss: 0.8342774058210438 | validation_loss: 0.7450318336486816\nEpoch: 2 | train_loss: 0.7265437594775496 | validation_loss: 0.7132484316825867\nEpoch: 3 | train_loss: 0.7079869262103377 | validation_loss: 0.7067291140556335\nEpoch: 4 | train_loss: 0.7021211015767065 | validation_loss: 0.650874924659729\nEpoch: 5 | train_loss: 0.7010712788022798 | validation_loss: 0.6359662413597107\nEpoch: 6 | train_loss: 0.6993205382906157 | validation_loss: 0.7385118007659912\nEpoch: 7 | train_loss: 0.6906545799354027 | validation_loss: 0.6630361159642537\nEpoch: 8 | train_loss: 0.6888790171721886 | validation_loss: 0.5630642890930175\nEpoch: 9 | train_loss: 0.6966121833899925 | validation_loss: 0.6487222274144491\nEpoch: 10 | train_loss: 0.686985433101654 | validation_loss: 0.7001470843950908\nEpoch: 11 | train_loss: 0.7026383445180696 | validation_loss: 0.6943316459655762\nEpoch: 12 | train_loss: 0.6789444417789064 | validation_loss: 0.6669096032778422\nEpoch: 13 | train_loss: 0.6917726253641063 | validation_loss: 0.7225844264030457\nEpoch: 14 | train_loss: 0.6817230767217176 | validation_loss: 0.6127895553906758\nEpoch: 15 | train_loss: 0.6852181348307379 | validation_loss: 0.7439935803413391\nEpoch: 16 | train_loss: 0.7017421167472313 | validation_loss: 0.7100629885991414\nEpoch: 17 | train_loss: 0.6929887409867912 | validation_loss: 0.6864541172981262\nEpoch: 18 | train_loss: 0.6679009404675714 | validation_loss: 0.5407335162162781\nEpoch: 19 | train_loss: 0.6261504025294863 | validation_loss: 0.6910412907600403\nEpoch: 20 | train_loss: 0.5605409268675179 | validation_loss: 0.6148863156636556\nEpoch: 21 | train_loss: 0.5421304569162172 | validation_loss: 0.7220795313517253\nEpoch: 22 | train_loss: 0.5284168530127098 | validation_loss: 0.5851226806640625\nEpoch: 23 | train_loss: 0.5212431560302603 | validation_loss: 0.43982014060020447\nEpoch: 24 | train_loss: 0.5036134940796885 | validation_loss: 0.3345524032910665\nEpoch: 25 | train_loss: 0.5318273151743 | validation_loss: 0.5234439969062805\nEpoch: 26 | train_loss: 0.4843400115596837 | validation_loss: 0.18941770493984222\nEpoch: 27 | train_loss: 0.4761212966565428 | validation_loss: 0.6933682203292847\nEpoch: 28 | train_loss: 0.483303416905732 | validation_loss: 0.48961468537648517\nEpoch: 29 | train_loss: 0.4617177116973647 | validation_loss: 0.5213870406150818\nEpoch: 30 | train_loss: 0.44885240752121497 | validation_loss: 0.32684156894683836\nEpoch: 31 | train_loss: 0.44253758109849073 | validation_loss: 0.261198761065801\nEpoch: 32 | train_loss: 0.4307012342173478 | validation_loss: 0.16652429103851318\nEpoch: 33 | train_loss: 0.41119736176112603 | validation_loss: 0.23402237792809805\nEpoch: 34 | train_loss: 0.4193988509219268 | validation_loss: 0.8301424980163574\nEpoch: 35 | train_loss: 0.40098050699151794 | validation_loss: 0.19016712605953218\nEpoch: 36 | train_loss: 0.4015096497946772 | validation_loss: 0.25715771317481995\nEpoch: 37 | train_loss: 0.39614317026631585 | validation_loss: 0.4363022744655609\nEpoch: 38 | train_loss: 0.39468295101461737 | validation_loss: 0.44366336266199746\nEpoch: 39 | train_loss: 0.393621656401404 | validation_loss: 0.6113824804623922\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\n# state_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_.pth')\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-21T03:04:24.560899Z","iopub.execute_input":"2023-04-21T03:04:24.561650Z","iopub.status.idle":"2023-04-21T03:04:24.618778Z","shell.execute_reply.started":"2023-04-21T03:04:24.561602Z","shell.execute_reply":"2023-04-21T03:04:24.617652Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"best epoch:  32\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'grp_norm_s.weight', 'grp_norm_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'dwconv1_t.bias', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.5.linear1.weight', 'Transf1_t.encoder.layers.5.linear1.bias', 'Transf1_t.encoder.layers.5.linear2.weight', 'Transf1_t.encoder.layers.5.linear2.bias', 'Transf1_t.encoder.layers.5.norm1.weight', 'Transf1_t.encoder.layers.5.norm1.bias', 'Transf1_t.encoder.layers.5.norm2.weight', 'Transf1_t.encoder.layers.5.norm2.bias', 'Transf1_t.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.6.linear1.weight', 'Transf1_t.encoder.layers.6.linear1.bias', 'Transf1_t.encoder.layers.6.linear2.weight', 'Transf1_t.encoder.layers.6.linear2.bias', 'Transf1_t.encoder.layers.6.norm1.weight', 'Transf1_t.encoder.layers.6.norm1.bias', 'Transf1_t.encoder.layers.6.norm2.weight', 'Transf1_t.encoder.layers.6.norm2.bias', 'Transf1_t.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.7.linear1.weight', 'Transf1_t.encoder.layers.7.linear1.bias', 'Transf1_t.encoder.layers.7.linear2.weight', 'Transf1_t.encoder.layers.7.linear2.bias', 'Transf1_t.encoder.layers.7.norm1.weight', 'Transf1_t.encoder.layers.7.norm1.bias', 'Transf1_t.encoder.layers.7.norm2.weight', 'Transf1_t.encoder.layers.7.norm2.bias', 'Transf1_t.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.8.linear1.weight', 'Transf1_t.encoder.layers.8.linear1.bias', 'Transf1_t.encoder.layers.8.linear2.weight', 'Transf1_t.encoder.layers.8.linear2.bias', 'Transf1_t.encoder.layers.8.norm1.weight', 'Transf1_t.encoder.layers.8.norm1.bias', 'Transf1_t.encoder.layers.8.norm2.weight', 'Transf1_t.encoder.layers.8.norm2.bias', 'Transf1_t.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.9.linear1.weight', 'Transf1_t.encoder.layers.9.linear1.bias', 'Transf1_t.encoder.layers.9.linear2.weight', 'Transf1_t.encoder.layers.9.linear2.bias', 'Transf1_t.encoder.layers.9.norm1.weight', 'Transf1_t.encoder.layers.9.norm1.bias', 'Transf1_t.encoder.layers.9.norm2.weight', 'Transf1_t.encoder.layers.9.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-21T03:04:24.620803Z","iopub.execute_input":"2023-04-21T03:04:24.621635Z","iopub.status.idle":"2023-04-21T03:04:26.951215Z","shell.execute_reply.started":"2023-04-21T03:04:24.621590Z","shell.execute_reply":"2023-04-21T03:04:26.949691Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-21T03:04:26.955867Z","iopub.execute_input":"2023-04-21T03:04:26.956765Z","iopub.status.idle":"2023-04-21T03:04:26.969334Z","shell.execute_reply.started":"2023-04-21T03:04:26.956708Z","shell.execute_reply":"2023-04-21T03:04:26.967641Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 4, 1)\n[[0.28823438]\n [0.7081403 ]\n [0.00228609]\n [0.00133919]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T03:04:26.971152Z","iopub.execute_input":"2023-04-21T03:04:26.972007Z","iopub.status.idle":"2023-04-21T03:04:26.985618Z","shell.execute_reply.started":"2023-04-21T03:04:26.971936Z","shell.execute_reply":"2023-04-21T03:04:26.984314Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T03:04:26.987536Z","iopub.execute_input":"2023-04-21T03:04:26.988192Z","iopub.status.idle":"2023-04-21T03:04:26.999465Z","shell.execute_reply.started":"2023-04-21T03:04:26.988147Z","shell.execute_reply":"2023-04-21T03:04:26.997952Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-21T03:04:27.001498Z","iopub.execute_input":"2023-04-21T03:04:27.002838Z","iopub.status.idle":"2023-04-21T03:04:27.503657Z","shell.execute_reply.started":"2023-04-21T03:04:27.002790Z","shell.execute_reply":"2023-04-21T03:04:27.502542Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"accuracy:  0.6666666666666666\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-21T03:04:27.505611Z","iopub.execute_input":"2023-04-21T03:04:27.506035Z","iopub.status.idle":"2023-04-21T03:04:27.544144Z","shell.execute_reply.started":"2023-04-21T03:04:27.505992Z","shell.execute_reply":"2023-04-21T03:04:27.543025Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}