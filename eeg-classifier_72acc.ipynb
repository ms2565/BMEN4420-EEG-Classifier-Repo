{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-20T02:16:15.508249Z","iopub.execute_input":"2023-04-20T02:16:15.508808Z","iopub.status.idle":"2023-04-20T02:16:15.514573Z","shell.execute_reply.started":"2023-04-20T02:16:15.508763Z","shell.execute_reply":"2023-04-20T02:16:15.513043Z"},"trusted":true},"execution_count":282,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-20T02:16:15.528932Z","iopub.execute_input":"2023-04-20T02:16:15.529388Z","iopub.status.idle":"2023-04-20T02:16:15.538885Z","shell.execute_reply.started":"2023-04-20T02:16:15.529343Z","shell.execute_reply":"2023-04-20T02:16:15.537574Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"# pip install oct2py\n#!apt-get install octave -y","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.547166Z","iopub.execute_input":"2023-04-20T02:16:15.548018Z","iopub.status.idle":"2023-04-20T02:16:15.553103Z","shell.execute_reply.started":"2023-04-20T02:16:15.547978Z","shell.execute_reply":"2023-04-20T02:16:15.551790Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.565747Z","iopub.execute_input":"2023-04-20T02:16:15.566124Z","iopub.status.idle":"2023-04-20T02:16:15.572181Z","shell.execute_reply.started":"2023-04-20T02:16:15.566086Z","shell.execute_reply":"2023-04-20T02:16:15.570780Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-20T02:16:15.582402Z","iopub.execute_input":"2023-04-20T02:16:15.583041Z","iopub.status.idle":"2023-04-20T02:16:15.590552Z","shell.execute_reply.started":"2023-04-20T02:16:15.583003Z","shell.execute_reply":"2023-04-20T02:16:15.589268Z"},"trusted":true},"execution_count":286,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.598952Z","iopub.execute_input":"2023-04-20T02:16:15.599324Z","iopub.status.idle":"2023-04-20T02:16:15.608017Z","shell.execute_reply.started":"2023-04-20T02:16:15.599263Z","shell.execute_reply":"2023-04-20T02:16:15.606688Z"},"trusted":true},"execution_count":287,"outputs":[{"execution_count":287,"output_type":"execute_result","data":{"text/plain":"['EEGPT_Models', '.virtual_documents', '__notebook_source__.ipynb']"},"metadata":{}}]},{"cell_type":"code","source":"datatype = 'eeg'","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.617922Z","iopub.execute_input":"2023-04-20T02:16:15.618374Z","iopub.status.idle":"2023-04-20T02:16:15.623607Z","shell.execute_reply.started":"2023-04-20T02:16:15.618335Z","shell.execute_reply":"2023-04-20T02:16:15.622336Z"},"trusted":true},"execution_count":288,"outputs":[]},{"cell_type":"code","source":"if datatype == 'eeg':\n    sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n    # sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n    # data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\nelif datatype == 'ica':\n    sub01 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub01.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub02.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub03.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub04.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub05.mat')\n    sub06 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub06.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub07.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub08.mat')\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-20T02:16:15.630524Z","iopub.execute_input":"2023-04-20T02:16:15.630895Z","iopub.status.idle":"2023-04-20T02:16:18.112679Z","shell.execute_reply.started":"2023-04-20T02:16:15.630860Z","shell.execute_reply":"2023-04-20T02:16:18.111351Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-20T02:16:18.115161Z","iopub.execute_input":"2023-04-20T02:16:18.115695Z","iopub.status.idle":"2023-04-20T02:16:18.124120Z","shell.execute_reply.started":"2023-04-20T02:16:18.115648Z","shell.execute_reply":"2023-04-20T02:16:18.122971Z"},"trusted":true},"execution_count":290,"outputs":[]},{"cell_type":"code","source":"class EEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(EEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n    self.AvgPool2_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n    self.AvgPool3_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=10,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=10,outSize=5,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=True, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1)) \n    self.conv2_t = nn.Conv1d(in_channels=eeg_channels//2,out_channels=eeg_channels//2, kernel_size=3, stride=1, bias = False, padding='same')\n    self.AvgPool2_t = nn.AvgPool2d(kernel_size=(2,1)) \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=60,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=60,outSize=5,numLayers=5,hiddenSize=5,numHeads=10,dropout=0.001)\n    # Build Fully Connected Path\n    if datatype == 'eeg':\n        self.fc1 = nn.Linear(1260,1)\n    elif datatype == 'ica':\n        self.fc1 = nn.Linear(1220,1)\n        \n\n  def forward(self, x):\n    # Spatial Pass\n    \n    x = x.to(torch.float32)\n#     print('x: ',x.shape)\n    x_s = self.Conv1_s(x)\n#     print('x conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x conv2: ',x_s.shape)\n    x_s = self.AvgPool2_s(x_s)\n#     print('x avg2: ',x_s.shape)\n    x_s = self.Conv3_s(x_s)\n#     print('x conv3: ',x_s.shape)\n    x_s = self.AvgPool3_s(x_s)\n    x_s = self.Conv4_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s_transf: ', x_s.shape)\n    \n    # Temporal Pass\n    #x_t = self.dwconv1_t(x)\n    #print('x_t conv1: ',x_t.shape)\n    #x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    #x_t = self.conv2_t(x_t)\n#     print('x_t conv2: ',x_t.shape)\n    #x_t = self.AvgPool2_t(x_t)\n#     print('x_t avg2: ',x_t.shape)\n    x_t = x.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n#     print('x_t avg1_permute: ',x_t.shape)    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t_transf: ', x_t.shape)\n    \n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n#     print('x_t transf1_perm: ',x_t.shape)\n#     print('x_s transf1_perm: ',x_s.shape)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n    # Output Pass: Fully Connected into Softmax\n#     print('x cat: ',x_cat.shape)\n    x = self.fc1(x_cat)\n#     print('x fc1: ',x.shape)\n    x = torch.log_softmax(x,dim=1)\n#     print('x softmax: ',x.shape)\n    return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-20T02:31:50.655112Z","iopub.execute_input":"2023-04-20T02:31:50.655512Z","iopub.status.idle":"2023-04-20T02:31:50.682437Z","shell.execute_reply.started":"2023-04-20T02:31:50.655477Z","shell.execute_reply":"2023-04-20T02:31:50.681343Z"},"trusted":true},"execution_count":348,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))\n\ndef mas2565_ICA(tensor):\n    pass\n    #return ICA_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:31:50.870129Z","iopub.execute_input":"2023-04-20T02:31:50.871420Z","iopub.status.idle":"2023-04-20T02:31:50.882928Z","shell.execute_reply.started":"2023-04-20T02:31:50.871378Z","shell.execute_reply":"2023-04-20T02:31:50.881807Z"},"trusted":true},"execution_count":349,"outputs":[]},{"cell_type":"code","source":"#print(data['sub01']['X_EEG_TRAIN'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:31:51.053047Z","iopub.execute_input":"2023-04-20T02:31:51.053426Z","iopub.status.idle":"2023-04-20T02:31:51.058313Z","shell.execute_reply.started":"2023-04-20T02:31:51.053391Z","shell.execute_reply":"2023-04-20T02:31:51.057195Z"},"trusted":true},"execution_count":350,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n    if i != 5:\n        subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n        subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n    for j in range(np.size(subSetY,0)):   \n        #print(np.shape(subSetX)[])\n        subx = subSetX[:,:,j]\n\n        #subx = mas2565_ICA(subx)\n        subx = mas2565_normalize(subx)\n        subx = mas2565_filter(subx)\n\n        #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n        #subx = noise(subx)\n        subx = torch.Tensor(subx)\n        #subx = mas2565_filter(subx)\n        #print(np.shape(subx))\n        suby = subSetY[j,:]\n        # miniSet = EEGData(subx,suby)\n        # print(np.shape(miniSet.y))\n        X.append(subx)\n        Y.append(suby)\n\n\n        # DEBUGGING PRINTS\n        #print(np.size(subSetY,0))\n        #print(np.shape(subSetX))\n        #print(np.shape(subSetY))\n        #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=3,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=3,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-20T02:31:51.232704Z","iopub.execute_input":"2023-04-20T02:31:51.232999Z","iopub.status.idle":"2023-04-20T02:31:52.714910Z","shell.execute_reply.started":"2023-04-20T02:31:51.232970Z","shell.execute_reply":"2023-04-20T02:31:52.713790Z"},"trusted":true},"execution_count":351,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = EEGPT(eeg_channels=60, time_len=1200)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.00005)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-20T02:31:52.717765Z","iopub.execute_input":"2023-04-20T02:31:52.718177Z","iopub.status.idle":"2023-04-20T02:31:52.753891Z","shell.execute_reply.started":"2023-04-20T02:31:52.718136Z","shell.execute_reply":"2023-04-20T02:31:52.752908Z"},"trusted":true},"execution_count":352,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-20T02:31:52.755478Z","iopub.execute_input":"2023-04-20T02:31:52.755999Z","iopub.status.idle":"2023-04-20T02:31:52.763406Z","shell.execute_reply.started":"2023-04-20T02:31:52.755953Z","shell.execute_reply":"2023-04-20T02:31:52.762050Z"},"trusted":true},"execution_count":353,"outputs":[{"name":"stdout","text":"number of model params:  267591\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\nEPOCHS = 50\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode\n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-20T02:31:52.765891Z","iopub.execute_input":"2023-04-20T02:31:52.766608Z","iopub.status.idle":"2023-04-20T02:35:59.245437Z","shell.execute_reply.started":"2023-04-20T02:31:52.766566Z","shell.execute_reply":"2023-04-20T02:35:59.244187Z"},"trusted":true},"execution_count":354,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 1.0562042867143948 | validation_loss: 0.9123188853263855\nEpoch: 1 | train_loss: 0.7130427500233054 | validation_loss: 1.0048494338989258\nEpoch: 2 | train_loss: 0.7109417458996177 | validation_loss: 0.897903561592102\nEpoch: 3 | train_loss: 0.6921376592169205 | validation_loss: 0.6428161263465881\nEpoch: 4 | train_loss: 0.6797203319147229 | validation_loss: 0.7149155735969543\nEpoch: 5 | train_loss: 0.6773188691586256 | validation_loss: 1.0549147129058838\nEpoch: 6 | train_loss: 0.6661483918627104 | validation_loss: 0.5898409485816956\nEpoch: 7 | train_loss: 0.63204952981323 | validation_loss: 0.34444567561149597\nEpoch: 8 | train_loss: 0.6311671785078943 | validation_loss: 0.8626402616500854\nEpoch: 9 | train_loss: 0.6266755918040872 | validation_loss: 0.6915735602378845\nEpoch: 10 | train_loss: 0.606845946672062 | validation_loss: 0.5392827987670898\nEpoch: 11 | train_loss: 0.5886575244367123 | validation_loss: 0.5754618644714355\nEpoch: 12 | train_loss: 0.5575757203623652 | validation_loss: 0.4335216283798218\nEpoch: 13 | train_loss: 0.546166741133978 | validation_loss: 0.21800628304481506\nEpoch: 14 | train_loss: 0.5326336262126764 | validation_loss: 0.28589871525764465\nEpoch: 15 | train_loss: 0.5160012105479836 | validation_loss: 0.22825607657432556\nEpoch: 16 | train_loss: 0.4869051619122426 | validation_loss: 0.4339469373226166\nEpoch: 17 | train_loss: 0.4638090383571883 | validation_loss: 0.20838424563407898\nEpoch: 18 | train_loss: 0.4337640697291742 | validation_loss: 0.4715020954608917\nEpoch: 19 | train_loss: 0.41636475548148155 | validation_loss: 0.6374106407165527\nEpoch: 20 | train_loss: 0.3872492096852511 | validation_loss: 0.3449472486972809\nEpoch: 21 | train_loss: 0.42799130202426267 | validation_loss: 0.8329258561134338\nEpoch: 22 | train_loss: 0.38664774879968417 | validation_loss: 0.13578394055366516\nEpoch: 23 | train_loss: 0.36927673127502203 | validation_loss: 0.22486145794391632\nEpoch: 24 | train_loss: 0.34623778770522523 | validation_loss: 0.1525266170501709\nEpoch: 25 | train_loss: 0.3570288038657357 | validation_loss: 0.1013321503996849\nEpoch: 26 | train_loss: 0.3282144252055635 | validation_loss: 0.756181001663208\nEpoch: 27 | train_loss: 0.32054330536630005 | validation_loss: 0.32496893405914307\nEpoch: 28 | train_loss: 0.2965556342775623 | validation_loss: 0.21262547373771667\nEpoch: 29 | train_loss: 0.2913824902304138 | validation_loss: 0.06418166309595108\nEpoch: 30 | train_loss: 0.27381801465526223 | validation_loss: 0.019822243601083755\nEpoch: 31 | train_loss: 0.268858529763141 | validation_loss: 0.13434742391109467\nEpoch: 32 | train_loss: 0.26467052643420175 | validation_loss: 0.1831585019826889\nEpoch: 33 | train_loss: 0.2520274800481275 | validation_loss: 0.08613321185112\nEpoch: 34 | train_loss: 0.24329114218320078 | validation_loss: 0.16184298694133759\nEpoch: 35 | train_loss: 0.22563212716098255 | validation_loss: 0.009357167407870293\nEpoch: 36 | train_loss: 0.20079726203888035 | validation_loss: 0.036031100898981094\nEpoch: 37 | train_loss: 0.19686329628651342 | validation_loss: 0.11312026530504227\nEpoch: 38 | train_loss: 0.18563755121431313 | validation_loss: 0.02587258815765381\nEpoch: 39 | train_loss: 0.16766643810357587 | validation_loss: 0.013171332888305187\nEpoch: 40 | train_loss: 0.15574230217801718 | validation_loss: 0.906141459941864\nEpoch: 41 | train_loss: 0.15733605307953744 | validation_loss: 0.04072902724146843\nEpoch: 42 | train_loss: 0.14579237644405416 | validation_loss: 0.07049696892499924\nEpoch: 43 | train_loss: 0.13163431831344496 | validation_loss: 0.22306515276432037\nEpoch: 44 | train_loss: 0.12361452910893907 | validation_loss: 0.06489479541778564\nEpoch: 45 | train_loss: 0.11134578450097858 | validation_loss: 0.19471271336078644\nEpoch: 46 | train_loss: 0.11312352637469303 | validation_loss: 0.040563859045505524\nEpoch: 47 | train_loss: 0.0898998979634295 | validation_loss: 0.0031014219857752323\nEpoch: 48 | train_loss: 0.07592388391397738 | validation_loss: 0.01480109617114067\nEpoch: 49 | train_loss: 0.08226003543192444 | validation_loss: 0.006577508524060249\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-20T02:35:59.247919Z","iopub.execute_input":"2023-04-20T02:35:59.248698Z","iopub.status.idle":"2023-04-20T02:35:59.297368Z","shell.execute_reply.started":"2023-04-20T02:35:59.248656Z","shell.execute_reply":"2023-04-20T02:35:59.296215Z"},"trusted":true},"execution_count":355,"outputs":[{"name":"stdout","text":"best epoch:  47\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'Conv3_s.weight', 'Conv3_s.bias', 'Conv4_s.weight', 'Conv4_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'dwconv1_t.bias', 'conv2_t.weight', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":355,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-20T02:35:59.299099Z","iopub.execute_input":"2023-04-20T02:35:59.299774Z","iopub.status.idle":"2023-04-20T02:36:01.147700Z","shell.execute_reply.started":"2023-04-20T02:35:59.299735Z","shell.execute_reply":"2023-04-20T02:36:01.146612Z"},"trusted":true},"execution_count":356,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-20T02:36:01.150518Z","iopub.execute_input":"2023-04-20T02:36:01.150916Z","iopub.status.idle":"2023-04-20T02:36:01.160154Z","shell.execute_reply.started":"2023-04-20T02:36:01.150873Z","shell.execute_reply":"2023-04-20T02:36:01.158722Z"},"trusted":true},"execution_count":357,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 5, 1)\n[[5.2525774e-03]\n [9.9465024e-01]\n [1.9412784e-05]\n [4.8578095e-05]\n [2.9140785e-05]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:36:01.162096Z","iopub.execute_input":"2023-04-20T02:36:01.162467Z","iopub.status.idle":"2023-04-20T02:36:01.175554Z","shell.execute_reply.started":"2023-04-20T02:36:01.162428Z","shell.execute_reply":"2023-04-20T02:36:01.173156Z"},"trusted":true},"execution_count":358,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:36:01.177411Z","iopub.execute_input":"2023-04-20T02:36:01.178774Z","iopub.status.idle":"2023-04-20T02:36:01.187509Z","shell.execute_reply.started":"2023-04-20T02:36:01.178736Z","shell.execute_reply":"2023-04-20T02:36:01.186415Z"},"trusted":true},"execution_count":359,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-20T02:36:01.189129Z","iopub.execute_input":"2023-04-20T02:36:01.189941Z","iopub.status.idle":"2023-04-20T02:36:01.478018Z","shell.execute_reply.started":"2023-04-20T02:36:01.189904Z","shell.execute_reply":"2023-04-20T02:36:01.476932Z"},"trusted":true},"execution_count":360,"outputs":[{"name":"stdout","text":"accuracy:  0.7163120567375887\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-20T02:20:33.144543Z","iopub.execute_input":"2023-04-20T02:20:33.145202Z","iopub.status.idle":"2023-04-20T02:20:33.206823Z","shell.execute_reply.started":"2023-04-20T02:20:33.145163Z","shell.execute_reply":"2023-04-20T02:20:33.205595Z"},"trusted":true},"execution_count":304,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}