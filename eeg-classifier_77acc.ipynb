{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-20T16:15:59.431578Z","iopub.execute_input":"2023-04-20T16:15:59.431976Z","iopub.status.idle":"2023-04-20T16:15:59.436295Z","shell.execute_reply.started":"2023-04-20T16:15:59.431939Z","shell.execute_reply":"2023-04-20T16:15:59.435275Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-20T16:15:59.477879Z","iopub.execute_input":"2023-04-20T16:15:59.478547Z","iopub.status.idle":"2023-04-20T16:15:59.485782Z","shell.execute_reply.started":"2023-04-20T16:15:59.478509Z","shell.execute_reply":"2023-04-20T16:15:59.484768Z"},"trusted":true},"execution_count":259,"outputs":[]},{"cell_type":"code","source":"# pip install oct2py\n#!apt-get install octave -y","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:15:59.507118Z","iopub.execute_input":"2023-04-20T16:15:59.507383Z","iopub.status.idle":"2023-04-20T16:15:59.511864Z","shell.execute_reply.started":"2023-04-20T16:15:59.507358Z","shell.execute_reply":"2023-04-20T16:15:59.510426Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:15:59.527668Z","iopub.execute_input":"2023-04-20T16:15:59.528277Z","iopub.status.idle":"2023-04-20T16:15:59.533783Z","shell.execute_reply.started":"2023-04-20T16:15:59.528240Z","shell.execute_reply":"2023-04-20T16:15:59.532742Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-20T16:15:59.542967Z","iopub.execute_input":"2023-04-20T16:15:59.543460Z","iopub.status.idle":"2023-04-20T16:15:59.549499Z","shell.execute_reply.started":"2023-04-20T16:15:59.543433Z","shell.execute_reply":"2023-04-20T16:15:59.548389Z"},"trusted":true},"execution_count":262,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:15:59.558005Z","iopub.execute_input":"2023-04-20T16:15:59.559779Z","iopub.status.idle":"2023-04-20T16:15:59.566312Z","shell.execute_reply.started":"2023-04-20T16:15:59.559748Z","shell.execute_reply":"2023-04-20T16:15:59.565215Z"},"trusted":true},"execution_count":263,"outputs":[{"execution_count":263,"output_type":"execute_result","data":{"text/plain":"['EEGPT_Models', '.virtual_documents', '__notebook_source__.ipynb']"},"metadata":{}}]},{"cell_type":"code","source":"datatype = 'eeg'","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:15:59.570603Z","iopub.execute_input":"2023-04-20T16:15:59.571335Z","iopub.status.idle":"2023-04-20T16:15:59.575920Z","shell.execute_reply.started":"2023-04-20T16:15:59.571304Z","shell.execute_reply":"2023-04-20T16:15:59.574790Z"},"trusted":true},"execution_count":264,"outputs":[]},{"cell_type":"code","source":"if datatype == 'eeg':\n    sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n    # sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n    # data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\nelif datatype == 'ica':\n    sub01 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub01.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub02.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub03.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub04.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub05.mat')\n    sub06 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub06.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub07.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub08.mat')\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-20T16:15:59.581818Z","iopub.execute_input":"2023-04-20T16:15:59.582896Z","iopub.status.idle":"2023-04-20T16:16:01.773451Z","shell.execute_reply.started":"2023-04-20T16:15:59.582860Z","shell.execute_reply":"2023-04-20T16:16:01.772334Z"},"trusted":true},"execution_count":265,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-20T16:16:01.775381Z","iopub.execute_input":"2023-04-20T16:16:01.776357Z","iopub.status.idle":"2023-04-20T16:16:01.783957Z","shell.execute_reply.started":"2023-04-20T16:16:01.776317Z","shell.execute_reply":"2023-04-20T16:16:01.783041Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"class testEEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(testEEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=17, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=10,stride=10)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=15,stride=1,padding=\"same\") # output should be \n    self.grp_norm_s = nn.GroupNorm(eeg_channels//6,eeg_channels)\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=time_len//10,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=time_len//10,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=False, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1))    \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels//2,max_length=1500)\n#     self.Transf1_t = EncoderTransformer(inSize=time_len,outSize=4,numLayers=3,hiddenSize=1,numHeads=6,dropout=0.01)\n    self.Transf1_t = EncoderTransformer(inSize=eeg_channels//2,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n    # Build Fully Connected Path\n    self.fc1 = nn.Linear(1260,1)\n\n  def forward(self, x):\n    # Spatial Pass\n    x_s = self.Conv1_s(x)\n#     print('x_s conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x_s avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x_s conv21: ',x_s.shape)\n    x_s = self.grp_norm_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s tf1: ',x_s.shape)\n    \n    # Temporal Pass\n    x_t = self.dwconv1_t(x)\n#     print('x_t conv1: ',x_t.shape)\n    x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    x_t = x_t.permute(0,2,1) # transpose to present time wise vectors to transformer encoder    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t tf1: ',x_t.shape)\n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n#     print('x_cat: ',x_cat.shape)\n    # Output Pass: Fully Connected into Softmax\n    x = self.fc1(x_cat)\n    x = torch.log_softmax(x,dim=1)\n    return x\n\nclass EEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(EEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n    self.AvgPool2_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n    self.AvgPool3_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=100,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=100,outSize=5,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=True, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1)) \n    self.conv2_t = nn.Conv1d(in_channels=eeg_channels//2,out_channels=eeg_channels//2, kernel_size=3, stride=1, bias = False, padding='same')\n    self.AvgPool2_t = nn.AvgPool2d(kernel_size=(2,1)) \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=60,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=60,outSize=5,numLayers=5,hiddenSize=5,numHeads=10,dropout=0.001)\n    # Build Fully Connected Path\n    if datatype == 'eeg':\n        self.fc1 = nn.Linear(1260,1)\n    elif datatype == 'ica':\n        self.fc1 = nn.Linear(1220,1)\n        \n\n  def forward(self, x):\n    # Spatial Pass\n    \n    x = x.to(torch.float32)\n#     print('x: ',x.shape)\n    x_s = self.Conv1_s(x)\n#     print('x conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x conv2: ',x_s.shape)\n#     x_s = self.AvgPool2_s(x_s)\n#     print('x avg2: ',x_s.shape)\n#     x_s = self.Conv3_s(x_s)\n#     print('x conv3: ',x_s.shape)\n#     x_s = self.AvgPool3_s(x_s)\n#     x_s = self.Conv4_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s_transf: ', x_s.shape)\n    \n    # Temporal Pass\n    #x_t = self.dwconv1_t(x)\n    #print('x_t conv1: ',x_t.shape)\n    #x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    #x_t = self.conv2_t(x_t)\n#     print('x_t conv2: ',x_t.shape)\n    #x_t = self.AvgPool2_t(x_t)\n#     print('x_t avg2: ',x_t.shape)\n    x_t = x.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n#     print('x_t avg1_permute: ',x_t.shape)    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t_transf: ', x_t.shape)\n    \n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n#     print('x_t transf1_perm: ',x_t.shape)\n#     print('x_s transf1_perm: ',x_s.shape)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n    # Output Pass: Fully Connected into Softmax\n#     print('x cat: ',x_cat.shape)\n    x = self.fc1(x_cat)\n#     print('x fc1: ',x.shape)\n    x = torch.log_softmax(x,dim=1)\n#     print('x softmax: ',x.shape)\n    return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-20T16:16:01.785828Z","iopub.execute_input":"2023-04-20T16:16:01.786221Z","iopub.status.idle":"2023-04-20T16:16:01.819004Z","shell.execute_reply.started":"2023-04-20T16:16:01.786184Z","shell.execute_reply":"2023-04-20T16:16:01.818112Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))\n\ndef mas2565_ICA(tensor):\n    pass\n    #return ICA_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:16:01.822204Z","iopub.execute_input":"2023-04-20T16:16:01.823110Z","iopub.status.idle":"2023-04-20T16:16:01.836117Z","shell.execute_reply.started":"2023-04-20T16:16:01.823068Z","shell.execute_reply":"2023-04-20T16:16:01.835196Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"#print(data['sub01']['X_EEG_TRAIN'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:16:01.837264Z","iopub.execute_input":"2023-04-20T16:16:01.837817Z","iopub.status.idle":"2023-04-20T16:16:01.850898Z","shell.execute_reply.started":"2023-04-20T16:16:01.837754Z","shell.execute_reply":"2023-04-20T16:16:01.849858Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n    if i != 5:\n        subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n        subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n    for j in range(np.size(subSetY,0)):   \n        #print(np.shape(subSetX)[])\n        subx = subSetX[:,:,j]\n\n        #subx = mas2565_ICA(subx)\n        subx = mas2565_normalize(subx)\n        subx = mas2565_filter(subx)\n\n        #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n        #subx = noise(subx)\n        subx = torch.Tensor(subx)\n        #subx = mas2565_filter(subx)\n        #print(np.shape(subx))\n        suby = subSetY[j,:]\n        # miniSet = EEGData(subx,suby)\n        # print(np.shape(miniSet.y))\n        X.append(subx)\n        Y.append(suby)\n\n\n        # DEBUGGING PRINTS\n        #print(np.size(subSetY,0))\n        #print(np.shape(subSetX))\n        #print(np.shape(subSetY))\n        #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=5,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=5,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-20T16:16:01.852441Z","iopub.execute_input":"2023-04-20T16:16:01.853276Z","iopub.status.idle":"2023-04-20T16:16:03.431437Z","shell.execute_reply.started":"2023-04-20T16:16:01.853239Z","shell.execute_reply":"2023-04-20T16:16:03.430340Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = testEEGPT(eeg_channels=60, time_len=1200)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.0001)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-20T16:16:24.558096Z","iopub.execute_input":"2023-04-20T16:16:24.559012Z","iopub.status.idle":"2023-04-20T16:16:24.601388Z","shell.execute_reply.started":"2023-04-20T16:16:24.558973Z","shell.execute_reply":"2023-04-20T16:16:24.600364Z"},"trusted":true},"execution_count":274,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-20T16:16:24.741021Z","iopub.execute_input":"2023-04-20T16:16:24.741828Z","iopub.status.idle":"2023-04-20T16:16:24.750433Z","shell.execute_reply.started":"2023-04-20T16:16:24.741785Z","shell.execute_reply":"2023-04-20T16:16:24.749199Z"},"trusted":true},"execution_count":275,"outputs":[{"name":"stdout","text":"number of model params:  812281\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\nEPOCHS = 40\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode\n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-20T16:16:24.913708Z","iopub.execute_input":"2023-04-20T16:16:24.914074Z","iopub.status.idle":"2023-04-20T16:19:25.702984Z","shell.execute_reply.started":"2023-04-20T16:16:24.914022Z","shell.execute_reply":"2023-04-20T16:19:25.701761Z"},"trusted":true},"execution_count":276,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 1.0147844552993774 | validation_loss: 0.884555995464325\nEpoch: 1 | train_loss: 0.72247884396849 | validation_loss: 0.9630682468414307\nEpoch: 2 | train_loss: 0.7248795710760971 | validation_loss: 0.8269515037536621\nEpoch: 3 | train_loss: 0.709556999905356 | validation_loss: 0.7906496524810791\nEpoch: 4 | train_loss: 0.7057943611309446 | validation_loss: 0.7623648643493652\nEpoch: 5 | train_loss: 0.700524532075586 | validation_loss: 0.8916264772415161\nEpoch: 6 | train_loss: 0.702136267875803 | validation_loss: 0.7806650996208191\nEpoch: 7 | train_loss: 0.7037840958299308 | validation_loss: 0.8618015646934509\nEpoch: 8 | train_loss: 0.7089633366157269 | validation_loss: 0.6834300756454468\nEpoch: 9 | train_loss: 0.7017053283494095 | validation_loss: 0.731874942779541\nEpoch: 10 | train_loss: 0.7066853221120506 | validation_loss: 0.8037748336791992\nEpoch: 11 | train_loss: 0.7014325785225836 | validation_loss: 0.6718899011611938\nEpoch: 12 | train_loss: 0.696437077275638 | validation_loss: 0.5150434374809265\nEpoch: 13 | train_loss: 0.7044371407607506 | validation_loss: 0.7203705310821533\nEpoch: 14 | train_loss: 0.7071897449164555 | validation_loss: 0.8280329704284668\nEpoch: 15 | train_loss: 0.6868778282198412 | validation_loss: 0.7882078886032104\nEpoch: 16 | train_loss: 0.6862441383559128 | validation_loss: 0.6687479615211487\nEpoch: 17 | train_loss: 0.6883828290577593 | validation_loss: 0.6747340559959412\nEpoch: 18 | train_loss: 0.6742036234715889 | validation_loss: 0.47498947381973267\nEpoch: 19 | train_loss: 0.6945923628478214 | validation_loss: 0.6390967965126038\nEpoch: 20 | train_loss: 0.671248744787841 | validation_loss: 0.5087991952896118\nEpoch: 21 | train_loss: 0.6903675085511701 | validation_loss: 0.5261716842651367\nEpoch: 22 | train_loss: 0.6768568086213079 | validation_loss: 0.4284498989582062\nEpoch: 23 | train_loss: 0.6707151655493111 | validation_loss: 0.9311078190803528\nEpoch: 24 | train_loss: 0.6413513072605791 | validation_loss: 0.3669103682041168\nEpoch: 25 | train_loss: 0.6753578067853533 | validation_loss: 0.3728654980659485\nEpoch: 26 | train_loss: 0.6232131406151015 | validation_loss: 0.6192314028739929\nEpoch: 27 | train_loss: 0.6511711709458252 | validation_loss: 0.7454937100410461\nEpoch: 28 | train_loss: 0.6383945479475218 | validation_loss: 0.6101115942001343\nEpoch: 29 | train_loss: 0.6575014950900242 | validation_loss: 0.41938066482543945\nEpoch: 30 | train_loss: 0.633859925742807 | validation_loss: 0.4327067732810974\nEpoch: 31 | train_loss: 0.5889344852546166 | validation_loss: 0.46572890877723694\nEpoch: 32 | train_loss: 0.5738934129476547 | validation_loss: 0.24906150996685028\nEpoch: 33 | train_loss: 0.5689423762518784 | validation_loss: 0.39847156405448914\nEpoch: 34 | train_loss: 0.589104655487784 | validation_loss: 0.4640997648239136\nEpoch: 35 | train_loss: 0.5607248236393106 | validation_loss: 0.22518062591552734\nEpoch: 36 | train_loss: 0.5798252178677197 | validation_loss: 0.7074943780899048\nEpoch: 37 | train_loss: 0.5938819500392881 | validation_loss: 0.3720829486846924\nEpoch: 38 | train_loss: 0.6152655179130619 | validation_loss: 0.3576974868774414\nEpoch: 39 | train_loss: 0.5718131902916678 | validation_loss: 0.1393251121044159\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\n# state_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_40.pth')\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-20T16:19:25.705410Z","iopub.execute_input":"2023-04-20T16:19:25.706555Z","iopub.status.idle":"2023-04-20T16:19:25.761774Z","shell.execute_reply.started":"2023-04-20T16:19:25.706506Z","shell.execute_reply":"2023-04-20T16:19:25.760588Z"},"trusted":true},"execution_count":277,"outputs":[{"name":"stdout","text":"best epoch:  39\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'grp_norm_s.weight', 'grp_norm_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.5.linear1.weight', 'Transf1_t.encoder.layers.5.linear1.bias', 'Transf1_t.encoder.layers.5.linear2.weight', 'Transf1_t.encoder.layers.5.linear2.bias', 'Transf1_t.encoder.layers.5.norm1.weight', 'Transf1_t.encoder.layers.5.norm1.bias', 'Transf1_t.encoder.layers.5.norm2.weight', 'Transf1_t.encoder.layers.5.norm2.bias', 'Transf1_t.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.6.linear1.weight', 'Transf1_t.encoder.layers.6.linear1.bias', 'Transf1_t.encoder.layers.6.linear2.weight', 'Transf1_t.encoder.layers.6.linear2.bias', 'Transf1_t.encoder.layers.6.norm1.weight', 'Transf1_t.encoder.layers.6.norm1.bias', 'Transf1_t.encoder.layers.6.norm2.weight', 'Transf1_t.encoder.layers.6.norm2.bias', 'Transf1_t.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.7.linear1.weight', 'Transf1_t.encoder.layers.7.linear1.bias', 'Transf1_t.encoder.layers.7.linear2.weight', 'Transf1_t.encoder.layers.7.linear2.bias', 'Transf1_t.encoder.layers.7.norm1.weight', 'Transf1_t.encoder.layers.7.norm1.bias', 'Transf1_t.encoder.layers.7.norm2.weight', 'Transf1_t.encoder.layers.7.norm2.bias', 'Transf1_t.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.8.linear1.weight', 'Transf1_t.encoder.layers.8.linear1.bias', 'Transf1_t.encoder.layers.8.linear2.weight', 'Transf1_t.encoder.layers.8.linear2.bias', 'Transf1_t.encoder.layers.8.norm1.weight', 'Transf1_t.encoder.layers.8.norm1.bias', 'Transf1_t.encoder.layers.8.norm2.weight', 'Transf1_t.encoder.layers.8.norm2.bias', 'Transf1_t.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.9.linear1.weight', 'Transf1_t.encoder.layers.9.linear1.bias', 'Transf1_t.encoder.layers.9.linear2.weight', 'Transf1_t.encoder.layers.9.linear2.bias', 'Transf1_t.encoder.layers.9.norm1.weight', 'Transf1_t.encoder.layers.9.norm1.bias', 'Transf1_t.encoder.layers.9.norm2.weight', 'Transf1_t.encoder.layers.9.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":277,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-20T16:19:25.763341Z","iopub.execute_input":"2023-04-20T16:19:25.763907Z","iopub.status.idle":"2023-04-20T16:19:28.236673Z","shell.execute_reply.started":"2023-04-20T16:19:25.763867Z","shell.execute_reply":"2023-04-20T16:19:28.235548Z"},"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-20T16:19:28.239283Z","iopub.execute_input":"2023-04-20T16:19:28.239624Z","iopub.status.idle":"2023-04-20T16:19:28.246337Z","shell.execute_reply.started":"2023-04-20T16:19:28.239593Z","shell.execute_reply":"2023-04-20T16:19:28.245219Z"},"trusted":true},"execution_count":279,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 4, 1)\n[[4.1001299e-01]\n [5.8993334e-01]\n [2.6190004e-05]\n [2.7481932e-05]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:19:28.247786Z","iopub.execute_input":"2023-04-20T16:19:28.249332Z","iopub.status.idle":"2023-04-20T16:19:28.266593Z","shell.execute_reply.started":"2023-04-20T16:19:28.249273Z","shell.execute_reply":"2023-04-20T16:19:28.265385Z"},"trusted":true},"execution_count":280,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:19:28.267855Z","iopub.execute_input":"2023-04-20T16:19:28.268740Z","iopub.status.idle":"2023-04-20T16:19:28.280442Z","shell.execute_reply.started":"2023-04-20T16:19:28.268703Z","shell.execute_reply":"2023-04-20T16:19:28.279124Z"},"trusted":true},"execution_count":281,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-20T16:19:28.282123Z","iopub.execute_input":"2023-04-20T16:19:28.282473Z","iopub.status.idle":"2023-04-20T16:19:28.575503Z","shell.execute_reply.started":"2023-04-20T16:19:28.282438Z","shell.execute_reply":"2023-04-20T16:19:28.574436Z"},"trusted":true},"execution_count":282,"outputs":[{"name":"stdout","text":"accuracy:  0.7730496453900709\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-20T16:19:28.577218Z","iopub.execute_input":"2023-04-20T16:19:28.577874Z","iopub.status.idle":"2023-04-20T16:19:28.612273Z","shell.execute_reply.started":"2023-04-20T16:19:28.577832Z","shell.execute_reply":"2023-04-20T16:19:28.611003Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}