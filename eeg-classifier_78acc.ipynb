{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-20T16:22:49.782295Z","iopub.execute_input":"2023-04-20T16:22:49.783234Z","iopub.status.idle":"2023-04-20T16:22:49.788549Z","shell.execute_reply.started":"2023-04-20T16:22:49.783181Z","shell.execute_reply":"2023-04-20T16:22:49.787112Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-20T16:22:49.801286Z","iopub.execute_input":"2023-04-20T16:22:49.801564Z","iopub.status.idle":"2023-04-20T16:22:49.810346Z","shell.execute_reply.started":"2023-04-20T16:22:49.801537Z","shell.execute_reply":"2023-04-20T16:22:49.809253Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"code","source":"# pip install oct2py\n#!apt-get install octave -y","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:22:49.814480Z","iopub.execute_input":"2023-04-20T16:22:49.814769Z","iopub.status.idle":"2023-04-20T16:22:49.825328Z","shell.execute_reply.started":"2023-04-20T16:22:49.814722Z","shell.execute_reply":"2023-04-20T16:22:49.824099Z"},"trusted":true},"execution_count":286,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:22:49.827675Z","iopub.execute_input":"2023-04-20T16:22:49.829407Z","iopub.status.idle":"2023-04-20T16:22:49.836049Z","shell.execute_reply.started":"2023-04-20T16:22:49.829377Z","shell.execute_reply":"2023-04-20T16:22:49.834929Z"},"trusted":true},"execution_count":287,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-20T16:22:49.844337Z","iopub.execute_input":"2023-04-20T16:22:49.845259Z","iopub.status.idle":"2023-04-20T16:22:49.852080Z","shell.execute_reply.started":"2023-04-20T16:22:49.845223Z","shell.execute_reply":"2023-04-20T16:22:49.850817Z"},"trusted":true},"execution_count":288,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:22:49.854456Z","iopub.execute_input":"2023-04-20T16:22:49.855316Z","iopub.status.idle":"2023-04-20T16:22:49.863241Z","shell.execute_reply.started":"2023-04-20T16:22:49.855278Z","shell.execute_reply":"2023-04-20T16:22:49.861961Z"},"trusted":true},"execution_count":289,"outputs":[{"execution_count":289,"output_type":"execute_result","data":{"text/plain":"['EEGPT_Models', '.virtual_documents', '__notebook_source__.ipynb']"},"metadata":{}}]},{"cell_type":"code","source":"datatype = 'eeg'","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:22:49.867485Z","iopub.execute_input":"2023-04-20T16:22:49.867777Z","iopub.status.idle":"2023-04-20T16:22:49.871960Z","shell.execute_reply.started":"2023-04-20T16:22:49.867749Z","shell.execute_reply":"2023-04-20T16:22:49.870961Z"},"trusted":true},"execution_count":290,"outputs":[]},{"cell_type":"code","source":"if datatype == 'eeg':\n    sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n    # sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n    # data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\nelif datatype == 'ica':\n    sub01 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub01.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub02.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub03.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub04.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub05.mat')\n    sub06 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub06.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub07.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub08.mat')\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-20T16:22:49.880192Z","iopub.execute_input":"2023-04-20T16:22:49.880517Z","iopub.status.idle":"2023-04-20T16:22:52.253217Z","shell.execute_reply.started":"2023-04-20T16:22:49.880491Z","shell.execute_reply":"2023-04-20T16:22:52.252127Z"},"trusted":true},"execution_count":291,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-20T16:22:52.255206Z","iopub.execute_input":"2023-04-20T16:22:52.255792Z","iopub.status.idle":"2023-04-20T16:22:52.264219Z","shell.execute_reply.started":"2023-04-20T16:22:52.255754Z","shell.execute_reply":"2023-04-20T16:22:52.263185Z"},"trusted":true},"execution_count":292,"outputs":[]},{"cell_type":"code","source":"class testEEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(testEEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=17, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=10,stride=10)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=15,stride=1,padding=\"same\") # output should be \n    self.grp_norm_s = nn.GroupNorm(eeg_channels//6,eeg_channels)\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=time_len//10,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=time_len//10,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=False, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1))    \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=eeg_channels//2,max_length=1500)\n#     self.Transf1_t = EncoderTransformer(inSize=time_len,outSize=4,numLayers=3,hiddenSize=1,numHeads=6,dropout=0.01)\n    self.Transf1_t = EncoderTransformer(inSize=eeg_channels//2,outSize=4,numLayers=10,hiddenSize=1,numHeads=6,dropout=0.01)\n    # Build Fully Connected Path\n    self.fc1 = nn.Linear(1260,1)\n\n  def forward(self, x):\n    # Spatial Pass\n    x_s = self.Conv1_s(x)\n#     print('x_s conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x_s avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x_s conv21: ',x_s.shape)\n    x_s = self.grp_norm_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s tf1: ',x_s.shape)\n    \n    # Temporal Pass\n    x_t = self.dwconv1_t(x)\n#     print('x_t conv1: ',x_t.shape)\n    x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    x_t = x_t.permute(0,2,1) # transpose to present time wise vectors to transformer encoder    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t tf1: ',x_t.shape)\n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n#     print('x_cat: ',x_cat.shape)\n    # Output Pass: Fully Connected into Softmax\n    x = self.fc1(x_cat)\n    x = torch.log_softmax(x,dim=1)\n    return x\n\nclass EEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(EEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n    self.AvgPool2_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n    self.AvgPool3_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=100,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=100,outSize=5,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=True, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1)) \n    self.conv2_t = nn.Conv1d(in_channels=eeg_channels//2,out_channels=eeg_channels//2, kernel_size=3, stride=1, bias = False, padding='same')\n    self.AvgPool2_t = nn.AvgPool2d(kernel_size=(2,1)) \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=60,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=60,outSize=5,numLayers=5,hiddenSize=5,numHeads=10,dropout=0.001)\n    # Build Fully Connected Path\n    if datatype == 'eeg':\n        self.fc1 = nn.Linear(1260,1)\n    elif datatype == 'ica':\n        self.fc1 = nn.Linear(1220,1)\n        \n\n  def forward(self, x):\n    # Spatial Pass\n    \n    x = x.to(torch.float32)\n#     print('x: ',x.shape)\n    x_s = self.Conv1_s(x)\n#     print('x conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x conv2: ',x_s.shape)\n#     x_s = self.AvgPool2_s(x_s)\n#     print('x avg2: ',x_s.shape)\n#     x_s = self.Conv3_s(x_s)\n#     print('x conv3: ',x_s.shape)\n#     x_s = self.AvgPool3_s(x_s)\n#     x_s = self.Conv4_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s_transf: ', x_s.shape)\n    \n    # Temporal Pass\n    #x_t = self.dwconv1_t(x)\n    #print('x_t conv1: ',x_t.shape)\n    #x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    #x_t = self.conv2_t(x_t)\n#     print('x_t conv2: ',x_t.shape)\n    #x_t = self.AvgPool2_t(x_t)\n#     print('x_t avg2: ',x_t.shape)\n    x_t = x.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n#     print('x_t avg1_permute: ',x_t.shape)    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t_transf: ', x_t.shape)\n    \n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n#     print('x_t transf1_perm: ',x_t.shape)\n#     print('x_s transf1_perm: ',x_s.shape)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n    # Output Pass: Fully Connected into Softmax\n#     print('x cat: ',x_cat.shape)\n    x = self.fc1(x_cat)\n#     print('x fc1: ',x.shape)\n    x = torch.log_softmax(x,dim=1)\n#     print('x softmax: ',x.shape)\n    return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-20T16:22:52.265676Z","iopub.execute_input":"2023-04-20T16:22:52.266249Z","iopub.status.idle":"2023-04-20T16:22:52.328316Z","shell.execute_reply.started":"2023-04-20T16:22:52.266213Z","shell.execute_reply":"2023-04-20T16:22:52.327125Z"},"trusted":true},"execution_count":293,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))\n\ndef mas2565_ICA(tensor):\n    pass\n    #return ICA_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:22:52.334474Z","iopub.execute_input":"2023-04-20T16:22:52.337178Z","iopub.status.idle":"2023-04-20T16:22:52.350128Z","shell.execute_reply.started":"2023-04-20T16:22:52.337138Z","shell.execute_reply":"2023-04-20T16:22:52.349078Z"},"trusted":true},"execution_count":294,"outputs":[]},{"cell_type":"code","source":"#print(data['sub01']['X_EEG_TRAIN'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:22:52.351662Z","iopub.execute_input":"2023-04-20T16:22:52.352339Z","iopub.status.idle":"2023-04-20T16:22:52.364509Z","shell.execute_reply.started":"2023-04-20T16:22:52.352295Z","shell.execute_reply":"2023-04-20T16:22:52.363322Z"},"trusted":true},"execution_count":295,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n    if i != 5:\n        subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n        subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n    for j in range(np.size(subSetY,0)):   \n        #print(np.shape(subSetX)[])\n        subx = subSetX[:,:,j]\n\n        #subx = mas2565_ICA(subx)\n        subx = mas2565_normalize(subx)\n        subx = mas2565_filter(subx)\n\n        #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n        #subx = noise(subx)\n        subx = torch.Tensor(subx)\n        #subx = mas2565_filter(subx)\n        #print(np.shape(subx))\n        suby = subSetY[j,:]\n        # miniSet = EEGData(subx,suby)\n        # print(np.shape(miniSet.y))\n        X.append(subx)\n        Y.append(suby)\n\n\n        # DEBUGGING PRINTS\n        #print(np.size(subSetY,0))\n        #print(np.shape(subSetX))\n        #print(np.shape(subSetY))\n        #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=10,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=10,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-20T16:22:52.365971Z","iopub.execute_input":"2023-04-20T16:22:52.366692Z","iopub.status.idle":"2023-04-20T16:22:54.260434Z","shell.execute_reply.started":"2023-04-20T16:22:52.366656Z","shell.execute_reply":"2023-04-20T16:22:54.259394Z"},"trusted":true},"execution_count":296,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = testEEGPT(eeg_channels=60, time_len=1200)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.0001)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-20T16:22:54.262477Z","iopub.execute_input":"2023-04-20T16:22:54.262866Z","iopub.status.idle":"2023-04-20T16:22:54.306145Z","shell.execute_reply.started":"2023-04-20T16:22:54.262825Z","shell.execute_reply":"2023-04-20T16:22:54.305217Z"},"trusted":true},"execution_count":297,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-20T16:22:54.307969Z","iopub.execute_input":"2023-04-20T16:22:54.309008Z","iopub.status.idle":"2023-04-20T16:22:54.316202Z","shell.execute_reply.started":"2023-04-20T16:22:54.308970Z","shell.execute_reply":"2023-04-20T16:22:54.314946Z"},"trusted":true},"execution_count":298,"outputs":[{"name":"stdout","text":"number of model params:  812281\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\nEPOCHS = 40\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode\n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-20T16:22:54.318130Z","iopub.execute_input":"2023-04-20T16:22:54.318534Z","iopub.status.idle":"2023-04-20T16:25:15.574535Z","shell.execute_reply.started":"2023-04-20T16:22:54.318493Z","shell.execute_reply":"2023-04-20T16:25:15.573353Z"},"trusted":true},"execution_count":299,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 1.1831549364944984 | validation_loss: 0.857839306195577\nEpoch: 1 | train_loss: 0.7396368240487987 | validation_loss: 0.6835929354031881\nEpoch: 2 | train_loss: 0.7374735680119745 | validation_loss: 0.6745559573173523\nEpoch: 3 | train_loss: 0.7035234858249796 | validation_loss: 0.7022185246149699\nEpoch: 4 | train_loss: 0.7030652761459351 | validation_loss: 0.6436906337738038\nEpoch: 5 | train_loss: 0.7084450660080746 | validation_loss: 0.7090047001838684\nEpoch: 6 | train_loss: 0.7082725393361059 | validation_loss: 0.7137028574943542\nEpoch: 7 | train_loss: 0.7124864212397871 | validation_loss: 0.8334474563598633\nEpoch: 8 | train_loss: 0.7045147480635807 | validation_loss: 0.7368597984313965\nEpoch: 9 | train_loss: 0.699053690351289 | validation_loss: 0.6879603266716003\nEpoch: 10 | train_loss: 0.6979104054385218 | validation_loss: 0.7241602937380472\nEpoch: 11 | train_loss: 0.7049670198868061 | validation_loss: 0.7231255491574605\nEpoch: 12 | train_loss: 0.7013997517783066 | validation_loss: 0.6993666052818298\nEpoch: 13 | train_loss: 0.70078772100909 | validation_loss: 0.7010910232861837\nEpoch: 14 | train_loss: 0.6979845371739618 | validation_loss: 0.7655505895614624\nEpoch: 15 | train_loss: 0.6959144781375753 | validation_loss: 0.6935752113660176\nEpoch: 16 | train_loss: 0.7016833588994783 | validation_loss: 0.6865218242009481\nEpoch: 17 | train_loss: 0.6970605377493233 | validation_loss: 0.8377613107363383\nEpoch: 18 | train_loss: 0.6940401463673033 | validation_loss: 0.7084769606590271\nEpoch: 19 | train_loss: 0.6981469536649769 | validation_loss: 0.6893844842910767\nEpoch: 20 | train_loss: 0.6930212378501892 | validation_loss: 0.7934861381848654\nEpoch: 21 | train_loss: 0.7008879061402946 | validation_loss: 0.7178278565406799\nEpoch: 22 | train_loss: 0.7027888277481342 | validation_loss: 0.6849214434623718\nEpoch: 23 | train_loss: 0.6919479493437142 | validation_loss: 0.6432437896728516\nEpoch: 24 | train_loss: 0.6886221869238491 | validation_loss: 0.616233766078949\nEpoch: 25 | train_loss: 0.6929246207763409 | validation_loss: 0.8282919526100159\nEpoch: 26 | train_loss: 0.6876013505047766 | validation_loss: 0.7523584206899007\nEpoch: 27 | train_loss: 0.671611789999337 | validation_loss: 0.6642417311668396\nEpoch: 28 | train_loss: 0.6326015571068073 | validation_loss: 0.6491779685020447\nEpoch: 29 | train_loss: 0.5805151380341629 | validation_loss: 0.37785854935646057\nEpoch: 30 | train_loss: 0.5466821717804876 | validation_loss: 0.664032518863678\nEpoch: 31 | train_loss: 0.5316045859764362 | validation_loss: 0.648602306842804\nEpoch: 32 | train_loss: 0.5296353749160109 | validation_loss: 0.6905756990114847\nEpoch: 33 | train_loss: 0.5121552132326981 | validation_loss: 0.48465577363967893\nEpoch: 34 | train_loss: 0.5207005163718914 | validation_loss: 0.3778821527957916\nEpoch: 35 | train_loss: 0.5181595015114752 | validation_loss: 0.458562171459198\nEpoch: 36 | train_loss: 0.5129914201539139 | validation_loss: 0.4150101959705353\nEpoch: 37 | train_loss: 0.4994089988798931 | validation_loss: 0.41090756058692934\nEpoch: 38 | train_loss: 0.5045548364006239 | validation_loss: 0.3960021734237671\nEpoch: 39 | train_loss: 0.5013754763479891 | validation_loss: 0.8574228008588155\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\n# state_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_40.pth')\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-20T16:25:15.578460Z","iopub.execute_input":"2023-04-20T16:25:15.578834Z","iopub.status.idle":"2023-04-20T16:25:15.634555Z","shell.execute_reply.started":"2023-04-20T16:25:15.578802Z","shell.execute_reply":"2023-04-20T16:25:15.633482Z"},"trusted":true},"execution_count":300,"outputs":[{"name":"stdout","text":"best epoch:  29\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'grp_norm_s.weight', 'grp_norm_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.5.linear1.weight', 'Transf1_t.encoder.layers.5.linear1.bias', 'Transf1_t.encoder.layers.5.linear2.weight', 'Transf1_t.encoder.layers.5.linear2.bias', 'Transf1_t.encoder.layers.5.norm1.weight', 'Transf1_t.encoder.layers.5.norm1.bias', 'Transf1_t.encoder.layers.5.norm2.weight', 'Transf1_t.encoder.layers.5.norm2.bias', 'Transf1_t.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.6.linear1.weight', 'Transf1_t.encoder.layers.6.linear1.bias', 'Transf1_t.encoder.layers.6.linear2.weight', 'Transf1_t.encoder.layers.6.linear2.bias', 'Transf1_t.encoder.layers.6.norm1.weight', 'Transf1_t.encoder.layers.6.norm1.bias', 'Transf1_t.encoder.layers.6.norm2.weight', 'Transf1_t.encoder.layers.6.norm2.bias', 'Transf1_t.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.7.linear1.weight', 'Transf1_t.encoder.layers.7.linear1.bias', 'Transf1_t.encoder.layers.7.linear2.weight', 'Transf1_t.encoder.layers.7.linear2.bias', 'Transf1_t.encoder.layers.7.norm1.weight', 'Transf1_t.encoder.layers.7.norm1.bias', 'Transf1_t.encoder.layers.7.norm2.weight', 'Transf1_t.encoder.layers.7.norm2.bias', 'Transf1_t.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.8.linear1.weight', 'Transf1_t.encoder.layers.8.linear1.bias', 'Transf1_t.encoder.layers.8.linear2.weight', 'Transf1_t.encoder.layers.8.linear2.bias', 'Transf1_t.encoder.layers.8.norm1.weight', 'Transf1_t.encoder.layers.8.norm1.bias', 'Transf1_t.encoder.layers.8.norm2.weight', 'Transf1_t.encoder.layers.8.norm2.bias', 'Transf1_t.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.9.linear1.weight', 'Transf1_t.encoder.layers.9.linear1.bias', 'Transf1_t.encoder.layers.9.linear2.weight', 'Transf1_t.encoder.layers.9.linear2.bias', 'Transf1_t.encoder.layers.9.norm1.weight', 'Transf1_t.encoder.layers.9.norm1.bias', 'Transf1_t.encoder.layers.9.norm2.weight', 'Transf1_t.encoder.layers.9.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":300,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-20T16:25:15.635865Z","iopub.execute_input":"2023-04-20T16:25:15.637299Z","iopub.status.idle":"2023-04-20T16:25:18.061673Z","shell.execute_reply.started":"2023-04-20T16:25:15.637262Z","shell.execute_reply":"2023-04-20T16:25:18.060479Z"},"trusted":true},"execution_count":301,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-20T16:25:18.063482Z","iopub.execute_input":"2023-04-20T16:25:18.063878Z","iopub.status.idle":"2023-04-20T16:25:18.071677Z","shell.execute_reply.started":"2023-04-20T16:25:18.063833Z","shell.execute_reply":"2023-04-20T16:25:18.070136Z"},"trusted":true},"execution_count":302,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 4, 1)\n[[1.1072483e-01]\n [8.8926560e-01]\n [4.2568727e-06]\n [5.3388662e-06]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:25:18.073640Z","iopub.execute_input":"2023-04-20T16:25:18.074087Z","iopub.status.idle":"2023-04-20T16:25:18.086311Z","shell.execute_reply.started":"2023-04-20T16:25:18.074022Z","shell.execute_reply":"2023-04-20T16:25:18.085175Z"},"trusted":true},"execution_count":303,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T16:25:18.088091Z","iopub.execute_input":"2023-04-20T16:25:18.088549Z","iopub.status.idle":"2023-04-20T16:25:18.100561Z","shell.execute_reply.started":"2023-04-20T16:25:18.088511Z","shell.execute_reply":"2023-04-20T16:25:18.099467Z"},"trusted":true},"execution_count":304,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-20T16:25:18.102140Z","iopub.execute_input":"2023-04-20T16:25:18.102785Z","iopub.status.idle":"2023-04-20T16:25:18.400708Z","shell.execute_reply.started":"2023-04-20T16:25:18.102747Z","shell.execute_reply":"2023-04-20T16:25:18.399611Z"},"trusted":true},"execution_count":305,"outputs":[{"name":"stdout","text":"accuracy:  0.7801418439716312\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-20T16:25:18.403371Z","iopub.execute_input":"2023-04-20T16:25:18.404221Z","iopub.status.idle":"2023-04-20T16:25:18.441169Z","shell.execute_reply.started":"2023-04-20T16:25:18.404179Z","shell.execute_reply":"2023-04-20T16:25:18.439947Z"},"trusted":true},"execution_count":306,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}