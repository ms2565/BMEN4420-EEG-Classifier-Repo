{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY3z4fGrPY0j","outputId":"b4b1b71e-3e35-462b-c095-f81f786878b1","execution":{"iopub.status.busy":"2023-04-20T02:16:15.508249Z","iopub.execute_input":"2023-04-20T02:16:15.508808Z","iopub.status.idle":"2023-04-20T02:16:15.514573Z","shell.execute_reply.started":"2023-04-20T02:16:15.508763Z","shell.execute_reply":"2023-04-20T02:16:15.513043Z"},"trusted":true},"execution_count":282,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport random\nimport scipy\nimport scipy.io as scio\nfrom scipy.signal import butter, sosfilt\nfrom scipy.stats import bernoulli\nfrom torch.utils.data import ConcatDataset, Dataset, DataLoader, random_split, RandomSampler\nimport numpy as np\n#from torchmetrics.classification import ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.preprocessing import normalize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from Models.Transformer import TransformerModel\n#from Models.PositionalEncoding import LearnedPositionalEncoding\n","metadata":{"id":"yhOLV8UPTrKb","execution":{"iopub.status.busy":"2023-04-20T02:16:15.528932Z","iopub.execute_input":"2023-04-20T02:16:15.529388Z","iopub.status.idle":"2023-04-20T02:16:15.538885Z","shell.execute_reply.started":"2023-04-20T02:16:15.529343Z","shell.execute_reply":"2023-04-20T02:16:15.537574Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"# pip install oct2py\n#!apt-get install octave -y","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.547166Z","iopub.execute_input":"2023-04-20T02:16:15.548018Z","iopub.status.idle":"2023-04-20T02:16:15.553103Z","shell.execute_reply.started":"2023-04-20T02:16:15.547978Z","shell.execute_reply":"2023-04-20T02:16:15.551790Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"code","source":"if (not(os.path.isdir('./EEGPT_Models'))):\n    os.makedirs('./EEGPT_Models')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.565747Z","iopub.execute_input":"2023-04-20T02:16:15.566124Z","iopub.status.idle":"2023-04-20T02:16:15.572181Z","shell.execute_reply.started":"2023-04-20T02:16:15.566086Z","shell.execute_reply":"2023-04-20T02:16:15.570780Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"code","source":"# CHECK GPU RESOURCES\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\ntorch.manual_seed(4460)# you don't have to set random seed beyond this block\nnp.random.seed(4460)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue7yaBP0kCW-","outputId":"81ec6b0e-0bfd-4e96-d60c-a3475b504e12","execution":{"iopub.status.busy":"2023-04-20T02:16:15.582402Z","iopub.execute_input":"2023-04-20T02:16:15.583041Z","iopub.status.idle":"2023-04-20T02:16:15.590552Z","shell.execute_reply.started":"2023-04-20T02:16:15.583003Z","shell.execute_reply":"2023-04-20T02:16:15.589268Z"},"trusted":true},"execution_count":286,"outputs":[{"name":"stdout","text":"GPU available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.598952Z","iopub.execute_input":"2023-04-20T02:16:15.599324Z","iopub.status.idle":"2023-04-20T02:16:15.608017Z","shell.execute_reply.started":"2023-04-20T02:16:15.599263Z","shell.execute_reply":"2023-04-20T02:16:15.606688Z"},"trusted":true},"execution_count":287,"outputs":[{"execution_count":287,"output_type":"execute_result","data":{"text/plain":"['EEGPT_Models', '.virtual_documents', '__notebook_source__.ipynb']"},"metadata":{}}]},{"cell_type":"code","source":"datatype = 'eeg'","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:16:15.617922Z","iopub.execute_input":"2023-04-20T02:16:15.618374Z","iopub.status.idle":"2023-04-20T02:16:15.623607Z","shell.execute_reply.started":"2023-04-20T02:16:15.618335Z","shell.execute_reply":"2023-04-20T02:16:15.622336Z"},"trusted":true},"execution_count":288,"outputs":[]},{"cell_type":"code","source":"if datatype == 'eeg':\n    sub01 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_1.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_2.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_3.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_4.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_5.mat')\n    # sub06 = scio.loadmat('/content/drive/MyDrive/Columbia Spring 2023/Signal Modeling/Project-EEG-Classifier/Signal_Processing_FC/Signal_Processing_FC/Subject_6.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_7.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-tensors/Signal_Processing_FC/Subject_8.mat')\n    # data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub07':sub07,'sub08':sub08}\nelif datatype == 'ica':\n    sub01 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub01.mat')\n    sub02 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub02.mat')\n    sub03 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub03.mat')\n    sub04 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub04.mat')\n    sub05 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub05.mat')\n    sub06 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub06.mat')\n    sub07 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub07.mat')\n    sub08 = scio.loadmat('/kaggle/input/eeg-ica-data-ii/ICA_EEG_Data/sub08.mat')\n    data = {'sub01':sub01,'sub02':sub02,'sub03':sub03,'sub04':sub04,'sub05':sub05,'sub06':sub06,'sub07':sub07,'sub08':sub08}","metadata":{"id":"lUT0FtKqgNPP","execution":{"iopub.status.busy":"2023-04-20T02:16:15.630524Z","iopub.execute_input":"2023-04-20T02:16:15.630895Z","iopub.status.idle":"2023-04-20T02:16:18.112679Z","shell.execute_reply.started":"2023-04-20T02:16:15.630860Z","shell.execute_reply":"2023-04-20T02:16:18.111351Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EEGData():\n  def __init__(self, samples, labels):\n    self.X = samples\n    self.Y = labels\n    self.indices = list(range(np.size(self.Y,0)))\n  def __getitem__(self, index):\n    eegTensor = X[index]\n    label = Y[index]    \n    sample = {'eeg' : eegTensor,\n              'label' : label}\n    return sample\n    #return self.x[self.indices[index]], self.y[self.indices[index]]\n  def shuffle(self):\n    random.shuffle(self.indices)\n  def __len__(self):\n    return (np.size(self.Y,0))","metadata":{"id":"CvUVk_oEw4CR","execution":{"iopub.status.busy":"2023-04-20T02:16:18.115161Z","iopub.execute_input":"2023-04-20T02:16:18.115695Z","iopub.status.idle":"2023-04-20T02:16:18.124120Z","shell.execute_reply.started":"2023-04-20T02:16:18.115648Z","shell.execute_reply":"2023-04-20T02:16:18.122971Z"},"trusted":true},"execution_count":290,"outputs":[]},{"cell_type":"code","source":"class EEGPT(nn.Module):\n  def __init__(\n      self,\n      eeg_channels = 60,\n      time_len = 1200\n               ):\n    super(EEGPT,self).__init__()\n    # BUILD SPATIAL PATH\n    ## CNN MODULE\n    self.Conv1_s = nn.Conv1d(in_channels=eeg_channels, out_channels=eeg_channels, kernel_size=16, stride=1, padding=\"same\")\n    self.AvgPool1_s = nn.AvgPool1d(kernel_size=4,stride=4)\n    self.Conv2_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=10,stride=1,padding=\"same\")\n    self.AvgPool2_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv3_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"same\")\n    self.AvgPool3_s = nn.AvgPool1d(kernel_size=3,stride=3)\n    self.Conv4_s = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels,kernel_size=9,stride=1,padding=\"valid\")\n    ## TRANSFORMER MODULE\n    self.PosEnc1_s = PositionalEncoder(embedding_dim=100,max_length=1000)\n    self.Transf1_s = EncoderTransformer(inSize=100,outSize=5,numLayers=10,hiddenSize=10,numHeads=10,dropout=0.001)\n\n    # BUILD TEMPORAL PATH\n    # CNN MODULE\n    self.dwconv1_t = nn.Conv1d(in_channels=eeg_channels,out_channels=eeg_channels, kernel_size=eeg_channels, stride=1, groups = eeg_channels, bias=True, padding=\"same\")\n    self.AvgPool1_t = nn.AvgPool2d(kernel_size=(2,1)) \n    self.conv2_t = nn.Conv1d(in_channels=eeg_channels//2,out_channels=eeg_channels//2, kernel_size=3, stride=1, bias = False, padding='same')\n    self.AvgPool2_t = nn.AvgPool2d(kernel_size=(2,1)) \n    # TRANSFORMER MODULE\n    self.PosEnc1_t = PositionalEncoder(embedding_dim=60,max_length=1500)\n    self.Transf1_t = EncoderTransformer(inSize=60,outSize=5,numLayers=5,hiddenSize=5,numHeads=10,dropout=0.001)\n    # Build Fully Connected Path\n    if datatype == 'eeg':\n        self.fc1 = nn.Linear(1260,1)\n    elif datatype == 'ica':\n        self.fc1 = nn.Linear(1220,1)\n        \n\n  def forward(self, x):\n    # Spatial Pass\n    \n    x = x.to(torch.float32)\n#     print('x: ',x.shape)\n    x_s = self.Conv1_s(x)\n#     print('x conv1: ',x_s.shape)\n    x_s = self.AvgPool1_s(x_s)\n#     print('x avg1: ',x_s.shape)\n    x_s = self.Conv2_s(x_s)\n#     print('x conv2: ',x_s.shape)\n    x_s = self.AvgPool2_s(x_s)\n#     print('x avg2: ',x_s.shape)\n    x_s = self.Conv3_s(x_s)\n#     print('x conv3: ',x_s.shape)\n#     x_s = self.AvgPool3_s(x_s)\n#     x_s = self.Conv4_s(x_s)\n    x_s = self.PosEnc1_s(x_s)\n    x_s = self.Transf1_s(x_s)\n#     print('x_s_transf: ', x_s.shape)\n    \n    # Temporal Pass\n    #x_t = self.dwconv1_t(x)\n    #print('x_t conv1: ',x_t.shape)\n    #x_t = self.AvgPool1_t(x_t)\n#     print('x_t avg1: ',x_t.shape)\n    #x_t = self.conv2_t(x_t)\n#     print('x_t conv2: ',x_t.shape)\n    #x_t = self.AvgPool2_t(x_t)\n#     print('x_t avg2: ',x_t.shape)\n    x_t = x.permute(0,2,1) # transpose to present time wise vectors to transformer encoder\n#     print('x_t avg1_permute: ',x_t.shape)    \n    x_t = self.PosEnc1_t(x_t)\n    x_t = self.Transf1_t(x_t)\n#     print('x_t_transf: ', x_t.shape)\n    \n    # Concatenation\n    x_s = x_s.permute(0,2,1)\n    x_t = x_t.permute(0,2,1)\n#     print('x_t transf1_perm: ',x_t.shape)\n#     print('x_s transf1_perm: ',x_s.shape)\n    x_cat = torch.cat((x_s, x_t),dim=2)\n    # Output Pass: Fully Connected into Softmax\n#     print('x cat: ',x_cat.shape)\n    x = self.fc1(x_cat)\n#     print('x fc1: ',x.shape)\n    x = torch.log_softmax(x,dim=1)\n#     print('x softmax: ',x.shape)\n    return x\n\nclass EncoderTransformer(nn.Module):\n  def __init__(self, inSize, outSize, numLayers=3, hiddenSize=1, numHeads=8, dropout=0.01):\n    super(EncoderTransformer,self).__init__()\n    self.encoderLayer = nn.TransformerEncoderLayer(d_model=inSize, nhead=numHeads, dim_feedforward=hiddenSize, dropout=dropout)\n    self.encoder = nn.TransformerEncoder(self.encoderLayer,num_layers=numLayers)\n    self.fc1 = nn.Linear(inSize, outSize)\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.fc1(x)\n    return x\n\n## CHECK HERE !\nclass PositionalEncoder(nn.Module):\n  def __init__(self, embedding_dim, max_length=1000):\n    super(PositionalEncoder,self).__init__()\n    pe = torch.zeros(max_length, embedding_dim)\n    position = torch.arange(0, max_length,dtype=float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embedding_dim, 2).float()\n        * (-torch.log(torch.tensor(10000.0))/embedding_dim)\n    )\n    pe[:,0::2] = torch.sin(position * div_term)\n    pe[:,1::2] = torch.cos(position * div_term)\n    pe.unsqueeze(0).transpose(0,1)\n    self.register_buffer('pe',pe)\n\n  def forward(self, x):\n    #print(self.pe[:x.size(1)].shape)\n    return x + self.pe[:x.size(1),:]\n\n","metadata":{"id":"IjLUvymIhn45","execution":{"iopub.status.busy":"2023-04-20T02:59:05.848520Z","iopub.execute_input":"2023-04-20T02:59:05.848941Z","iopub.status.idle":"2023-04-20T02:59:05.872408Z","shell.execute_reply.started":"2023-04-20T02:59:05.848906Z","shell.execute_reply":"2023-04-20T02:59:05.871242Z"},"trusted":true},"execution_count":467,"outputs":[]},{"cell_type":"code","source":"# PREPROCESSING FUNCTIONS\n#tensor = subx\n#print(np.shape(subx))\nclass AddGaussNoise(object):\n    def __init__(self, std, mean, p):\n        self.std = std\n        self.mean = mean\n        self.prob = p # tune probability controlling fraction of dataset this augmentation will be applied to\n    def __call__(self, tensor):\n        #return img + torch.randn_like(img)*std + mean\n        bern_rv = bernoulli.rvs(self.prob)\n        if bern_rv == 1:\n            ret_tensor = tensor + np.random.randn(np.shape(tensor)[0],np.shape(tensor)[1])*self.std + self.mean\n        else:\n            ret_tensor = tensor                \n        return ret_tensor \n\ndef mas2565_normalize(tensor):\n    # normalizes a 60 x 1200 tensor, time wise\n    normal_tensor = normalize(tensor,axis=1,norm='l2')\n    return normal_tensor\ndef mas2565_filter(tensor):\n    Fs = 1000\n    lowcut = 0.5\n    highcut = 40\n    order = 4\n    nyq = 0.5*Fs\n    low = lowcut/nyq\n    high = highcut/nyq\n    sos = butter(order, [low, high], btype='band',output='sos')\n    filtered_tensor = sosfilt(sos, tensor, axis=1)\n    return filtered_tensor\n#print(np.shape(mas2565_normalize(tensor)))\n\ndef mas2565_ICA(tensor):\n    pass\n    #return ICA_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:59:08.567828Z","iopub.execute_input":"2023-04-20T02:59:08.568483Z","iopub.status.idle":"2023-04-20T02:59:08.578380Z","shell.execute_reply.started":"2023-04-20T02:59:08.568431Z","shell.execute_reply":"2023-04-20T02:59:08.577227Z"},"trusted":true},"execution_count":468,"outputs":[]},{"cell_type":"code","source":"#print(data['sub01']['X_EEG_TRAIN'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T02:59:08.771318Z","iopub.execute_input":"2023-04-20T02:59:08.771894Z","iopub.status.idle":"2023-04-20T02:59:08.776606Z","shell.execute_reply.started":"2023-04-20T02:59:08.771862Z","shell.execute_reply":"2023-04-20T02:59:08.775517Z"},"trusted":true},"execution_count":469,"outputs":[]},{"cell_type":"code","source":"# COMPOSE MEGA DATASET FROM ALL SUBJECT TENSORS\nnumSets = 8\nX = []\nY = []\nID = []\nfor i in range(numSets):\n    if i != 5:\n        subSetX = data[('sub0'+str(i+1))]['X_EEG_TRAIN']\n        subSetY = data[('sub0'+str(i+1))]['Y_EEG_TRAIN']\n  #print(np.size(subSetY,0))\n    for j in range(np.size(subSetY,0)):   \n        #print(np.shape(subSetX)[])\n        subx = subSetX[:,:,j]\n\n        #subx = mas2565_ICA(subx)\n        subx = mas2565_normalize(subx)\n        subx = mas2565_filter(subx)\n\n        #noise = AddGaussNoise(50,0,0.7) # noise augmentation\n        #subx = noise(subx)\n        subx = torch.Tensor(subx)\n        #subx = mas2565_filter(subx)\n        #print(np.shape(subx))\n        suby = subSetY[j,:]\n        # miniSet = EEGData(subx,suby)\n        # print(np.shape(miniSet.y))\n        X.append(subx)\n        Y.append(suby)\n\n\n        # DEBUGGING PRINTS\n        #print(np.size(subSetY,0))\n        #print(np.shape(subSetX))\n        #print(np.shape(subSetY))\n        #print(miniSet.__len__())\n\n#MegaSet = ConcatDataset(megaSet)\n#print(np.shape((MegaSet).x))\n#MegaSet = RandomSampler(MegaSet)\n#print(np.shape(X))\n#print(np.shape(Y[1]))\n\nmyEEG = EEGData(X,Y)\n\n# Load Dataset using EEGData and Dataloader\ntrainset, validset, testset = random_split(myEEG,[0.5, 0.25, 0.25])\ntrainloader = DataLoader(trainset,batch_size=3,shuffle=True)\nvalidloader = DataLoader(validset,batch_size=3,shuffle=True)\ntestloader = DataLoader(testset, batch_size =1, shuffle=True)","metadata":{"id":"2tat7z1h7fPw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b710ee-f7b9-417c-b27a-0eb1a60b8946","execution":{"iopub.status.busy":"2023-04-20T02:59:08.981210Z","iopub.execute_input":"2023-04-20T02:59:08.981989Z","iopub.status.idle":"2023-04-20T02:59:10.449790Z","shell.execute_reply.started":"2023-04-20T02:59:08.981953Z","shell.execute_reply":"2023-04-20T02:59:10.448659Z"},"trusted":true},"execution_count":470,"outputs":[]},{"cell_type":"code","source":"# Build/Instantiate Model\neegpt = EEGPT(eeg_channels=60, time_len=1200)\nif cuda:\n  eegpt.cuda()\n\n# Call Optimizer\nadam = Adam(eegpt.parameters(),lr=0.0001)","metadata":{"id":"u8WNB1li-GX0","execution":{"iopub.status.busy":"2023-04-20T02:59:10.452544Z","iopub.execute_input":"2023-04-20T02:59:10.452924Z","iopub.status.idle":"2023-04-20T02:59:10.488705Z","shell.execute_reply.started":"2023-04-20T02:59:10.452881Z","shell.execute_reply":"2023-04-20T02:59:10.487744Z"},"trusted":true},"execution_count":471,"outputs":[]},{"cell_type":"code","source":"# COUNT MODEL PARAMETERS\nparam_count = 0;\nfor param in eegpt.parameters():\n    param_count += param.numel()\n\nprint('number of model params: ', param_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_dPdRf_hV-m","outputId":"0c4744ed-0b2f-41d4-d3b0-6a09563a2318","execution":{"iopub.status.busy":"2023-04-20T02:59:10.490322Z","iopub.execute_input":"2023-04-20T02:59:10.490700Z","iopub.status.idle":"2023-04-20T02:59:10.497732Z","shell.execute_reply.started":"2023-04-20T02:59:10.490664Z","shell.execute_reply":"2023-04-20T02:59:10.496526Z"},"trusted":true},"execution_count":472,"outputs":[{"name":"stdout","text":"number of model params:  732351\n","output_type":"stream"}]},{"cell_type":"code","source":"# MODEL TRAINING\nEPOCHS = 50\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\nfor epoch in range(EPOCHS):\n  train_loss = list()\n  valid_loss = list()\n  eegpt.train() # put model in train mode\n  for i, sample in enumerate(trainloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print('label shape: ',np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      train_pred = eegpt(eegTensor.cuda())\n      # print('pred shape: ', train_pred.shape)\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      train_loss.append(loss.cpu().data.item())\n      # reset gradient\n      adam.zero_grad()\n      # back propagation\n      loss.backward()\n      # Update parameters\n      adam.step()\n      #print('epoch: ', epoch, ' loss: ', loss.item())\n      \n      #print(f'EPOCH {epoch + 1}/{EPOCHS} - Training Batch {i+1}/{len(trainloader)} - Loss: {loss.item()}', end='\\r')\n  eegpt.eval()\n  for i, samples in enumerate(validloader):\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    if cuda:\n      valid_pred = eegpt(eegTensor.cuda())\n      # calculate loss\n      loss_fun = nn.CrossEntropyLoss()\n      loss = loss_fun(train_pred, label.cuda().long())\n      valid_loss.append(loss.cpu().data.item())\n      \n  train_epoch_loss.append(np.mean(train_loss))\n  validation_epoch_loss.append(np.mean(valid_loss))\n  print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))\n  # print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n  torch.save(eegpt.state_dict(), '/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (epoch))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"79_uGinHjAXm","outputId":"631006fb-42d7-4895-b1f7-350db5497e1a","execution":{"iopub.status.busy":"2023-04-20T02:59:10.668905Z","iopub.execute_input":"2023-04-20T02:59:10.670515Z","iopub.status.idle":"2023-04-20T03:03:31.554579Z","shell.execute_reply.started":"2023-04-20T02:59:10.670476Z","shell.execute_reply":"2023-04-20T03:03:31.553446Z"},"trusted":true},"execution_count":473,"outputs":[{"name":"stdout","text":"Epoch: 0 | train_loss: 0.8692300173764428 | validation_loss: 0.35078081488609314\nEpoch: 1 | train_loss: 0.7188450898975134 | validation_loss: 0.8225061893463135\nEpoch: 2 | train_loss: 0.7119912647952636 | validation_loss: 0.6012350916862488\nEpoch: 3 | train_loss: 0.7257345775142312 | validation_loss: 0.761969268321991\nEpoch: 4 | train_loss: 0.7090245836103956 | validation_loss: 0.6975765824317932\nEpoch: 5 | train_loss: 0.6754844151437283 | validation_loss: 0.25973743200302124\nEpoch: 6 | train_loss: 0.6763127806286017 | validation_loss: 0.9212157726287842\nEpoch: 7 | train_loss: 0.6788817745012542 | validation_loss: 0.7510135173797607\nEpoch: 8 | train_loss: 0.6618132999477288 | validation_loss: 0.48800480365753174\nEpoch: 9 | train_loss: 0.6368619271864494 | validation_loss: 0.8852407932281494\nEpoch: 10 | train_loss: 0.6095839600699643 | validation_loss: 0.2849571108818054\nEpoch: 11 | train_loss: 0.5933619295246899 | validation_loss: 0.4162069261074066\nEpoch: 12 | train_loss: 0.5760555909946561 | validation_loss: 0.37752246856689453\nEpoch: 13 | train_loss: 0.5439555703972777 | validation_loss: 0.10094481706619263\nEpoch: 14 | train_loss: 0.5735505695144335 | validation_loss: 0.9052271842956543\nEpoch: 15 | train_loss: 0.5302278205441932 | validation_loss: 0.5377727746963501\nEpoch: 16 | train_loss: 0.5450503976705173 | validation_loss: 0.9748325347900391\nEpoch: 17 | train_loss: 0.5314728439164659 | validation_loss: 0.7413058280944824\nEpoch: 18 | train_loss: 0.5021681050614765 | validation_loss: 0.7673774361610413\nEpoch: 19 | train_loss: 0.5306531658861786 | validation_loss: 1.3792979717254639\nEpoch: 20 | train_loss: 0.5309145842523625 | validation_loss: 0.13990849256515503\nEpoch: 21 | train_loss: 0.4918279855822523 | validation_loss: 0.14633174240589142\nEpoch: 22 | train_loss: 0.5086743298452348 | validation_loss: 0.4113273024559021\nEpoch: 23 | train_loss: 0.4867342590975265 | validation_loss: 0.09948064386844635\nEpoch: 24 | train_loss: 0.5062752409915751 | validation_loss: 0.18166135251522064\nEpoch: 25 | train_loss: 0.4946174155920744 | validation_loss: 1.1582043170928955\nEpoch: 26 | train_loss: 0.4866259527237465 | validation_loss: 0.07685434073209763\nEpoch: 27 | train_loss: 0.4298207429625715 | validation_loss: 0.2980119287967682\nEpoch: 28 | train_loss: 0.4652452440156291 | validation_loss: 0.46551674604415894\nEpoch: 29 | train_loss: 0.4280053713203718 | validation_loss: 1.1491971015930176\nEpoch: 30 | train_loss: 0.4452029178695132 | validation_loss: 0.6819435954093933\nEpoch: 31 | train_loss: 0.4322265679948032 | validation_loss: 0.13132110238075256\nEpoch: 32 | train_loss: 0.4241140369558707 | validation_loss: 0.16368134319782257\nEpoch: 33 | train_loss: 0.42212691468497116 | validation_loss: 0.4003695547580719\nEpoch: 34 | train_loss: 0.37441255324908224 | validation_loss: 0.33424094319343567\nEpoch: 35 | train_loss: 0.4093464359369439 | validation_loss: 0.3432287871837616\nEpoch: 36 | train_loss: 0.353626368722568 | validation_loss: 0.24421215057373047\nEpoch: 37 | train_loss: 0.3671760072465986 | validation_loss: 0.340810626745224\nEpoch: 38 | train_loss: 0.3928200606023893 | validation_loss: 0.05387817323207855\nEpoch: 39 | train_loss: 0.3471881495594668 | validation_loss: 0.8209874629974365\nEpoch: 40 | train_loss: 0.304948554248161 | validation_loss: 1.6955151557922363\nEpoch: 41 | train_loss: 0.3181582289495661 | validation_loss: 2.1362531185150146\nEpoch: 42 | train_loss: 0.3161223984886116 | validation_loss: 0.4803944230079651\nEpoch: 43 | train_loss: 0.2974341745333125 | validation_loss: 0.09189716726541519\nEpoch: 44 | train_loss: 0.2985667970982225 | validation_loss: 0.014507290907204151\nEpoch: 45 | train_loss: 0.29332516554859467 | validation_loss: 0.5855440497398376\nEpoch: 46 | train_loss: 0.2962004233656141 | validation_loss: 0.2126685529947281\nEpoch: 47 | train_loss: 0.28031729222857393 | validation_loss: 0.05125618353486061\nEpoch: 48 | train_loss: 0.28483940882142633 | validation_loss: 0.21402734518051147\nEpoch: 49 | train_loss: 0.24374905663232008 | validation_loss: 0.12441214919090271\n","output_type":"stream"}]},{"cell_type":"code","source":"# BEST EPOCH\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)\n\n# LOAD BEST MODEL\nstate_dict = torch.load('/kaggle/working/EEGPT_Models/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\neegpt.load_state_dict(state_dict)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN5zN2vCXeVD","outputId":"5af53d39-727c-4d01-ecc5-c8e4bb86d42f","execution":{"iopub.status.busy":"2023-04-20T03:03:31.560712Z","iopub.execute_input":"2023-04-20T03:03:31.564334Z","iopub.status.idle":"2023-04-20T03:03:31.647157Z","shell.execute_reply.started":"2023-04-20T03:03:31.564252Z","shell.execute_reply":"2023-04-20T03:03:31.644130Z"},"trusted":true},"execution_count":474,"outputs":[{"name":"stdout","text":"best epoch:  44\nodict_keys(['Conv1_s.weight', 'Conv1_s.bias', 'Conv2_s.weight', 'Conv2_s.bias', 'Conv3_s.weight', 'Conv3_s.bias', 'Conv4_s.weight', 'Conv4_s.bias', 'PosEnc1_s.pe', 'Transf1_s.encoderLayer.self_attn.in_proj_weight', 'Transf1_s.encoderLayer.self_attn.in_proj_bias', 'Transf1_s.encoderLayer.self_attn.out_proj.weight', 'Transf1_s.encoderLayer.self_attn.out_proj.bias', 'Transf1_s.encoderLayer.linear1.weight', 'Transf1_s.encoderLayer.linear1.bias', 'Transf1_s.encoderLayer.linear2.weight', 'Transf1_s.encoderLayer.linear2.bias', 'Transf1_s.encoderLayer.norm1.weight', 'Transf1_s.encoderLayer.norm1.bias', 'Transf1_s.encoderLayer.norm2.weight', 'Transf1_s.encoderLayer.norm2.bias', 'Transf1_s.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.0.linear1.weight', 'Transf1_s.encoder.layers.0.linear1.bias', 'Transf1_s.encoder.layers.0.linear2.weight', 'Transf1_s.encoder.layers.0.linear2.bias', 'Transf1_s.encoder.layers.0.norm1.weight', 'Transf1_s.encoder.layers.0.norm1.bias', 'Transf1_s.encoder.layers.0.norm2.weight', 'Transf1_s.encoder.layers.0.norm2.bias', 'Transf1_s.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.1.linear1.weight', 'Transf1_s.encoder.layers.1.linear1.bias', 'Transf1_s.encoder.layers.1.linear2.weight', 'Transf1_s.encoder.layers.1.linear2.bias', 'Transf1_s.encoder.layers.1.norm1.weight', 'Transf1_s.encoder.layers.1.norm1.bias', 'Transf1_s.encoder.layers.1.norm2.weight', 'Transf1_s.encoder.layers.1.norm2.bias', 'Transf1_s.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.2.linear1.weight', 'Transf1_s.encoder.layers.2.linear1.bias', 'Transf1_s.encoder.layers.2.linear2.weight', 'Transf1_s.encoder.layers.2.linear2.bias', 'Transf1_s.encoder.layers.2.norm1.weight', 'Transf1_s.encoder.layers.2.norm1.bias', 'Transf1_s.encoder.layers.2.norm2.weight', 'Transf1_s.encoder.layers.2.norm2.bias', 'Transf1_s.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.3.linear1.weight', 'Transf1_s.encoder.layers.3.linear1.bias', 'Transf1_s.encoder.layers.3.linear2.weight', 'Transf1_s.encoder.layers.3.linear2.bias', 'Transf1_s.encoder.layers.3.norm1.weight', 'Transf1_s.encoder.layers.3.norm1.bias', 'Transf1_s.encoder.layers.3.norm2.weight', 'Transf1_s.encoder.layers.3.norm2.bias', 'Transf1_s.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.4.linear1.weight', 'Transf1_s.encoder.layers.4.linear1.bias', 'Transf1_s.encoder.layers.4.linear2.weight', 'Transf1_s.encoder.layers.4.linear2.bias', 'Transf1_s.encoder.layers.4.norm1.weight', 'Transf1_s.encoder.layers.4.norm1.bias', 'Transf1_s.encoder.layers.4.norm2.weight', 'Transf1_s.encoder.layers.4.norm2.bias', 'Transf1_s.encoder.layers.5.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.5.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.5.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.5.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.5.linear1.weight', 'Transf1_s.encoder.layers.5.linear1.bias', 'Transf1_s.encoder.layers.5.linear2.weight', 'Transf1_s.encoder.layers.5.linear2.bias', 'Transf1_s.encoder.layers.5.norm1.weight', 'Transf1_s.encoder.layers.5.norm1.bias', 'Transf1_s.encoder.layers.5.norm2.weight', 'Transf1_s.encoder.layers.5.norm2.bias', 'Transf1_s.encoder.layers.6.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.6.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.6.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.6.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.6.linear1.weight', 'Transf1_s.encoder.layers.6.linear1.bias', 'Transf1_s.encoder.layers.6.linear2.weight', 'Transf1_s.encoder.layers.6.linear2.bias', 'Transf1_s.encoder.layers.6.norm1.weight', 'Transf1_s.encoder.layers.6.norm1.bias', 'Transf1_s.encoder.layers.6.norm2.weight', 'Transf1_s.encoder.layers.6.norm2.bias', 'Transf1_s.encoder.layers.7.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.7.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.7.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.7.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.7.linear1.weight', 'Transf1_s.encoder.layers.7.linear1.bias', 'Transf1_s.encoder.layers.7.linear2.weight', 'Transf1_s.encoder.layers.7.linear2.bias', 'Transf1_s.encoder.layers.7.norm1.weight', 'Transf1_s.encoder.layers.7.norm1.bias', 'Transf1_s.encoder.layers.7.norm2.weight', 'Transf1_s.encoder.layers.7.norm2.bias', 'Transf1_s.encoder.layers.8.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.8.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.8.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.8.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.8.linear1.weight', 'Transf1_s.encoder.layers.8.linear1.bias', 'Transf1_s.encoder.layers.8.linear2.weight', 'Transf1_s.encoder.layers.8.linear2.bias', 'Transf1_s.encoder.layers.8.norm1.weight', 'Transf1_s.encoder.layers.8.norm1.bias', 'Transf1_s.encoder.layers.8.norm2.weight', 'Transf1_s.encoder.layers.8.norm2.bias', 'Transf1_s.encoder.layers.9.self_attn.in_proj_weight', 'Transf1_s.encoder.layers.9.self_attn.in_proj_bias', 'Transf1_s.encoder.layers.9.self_attn.out_proj.weight', 'Transf1_s.encoder.layers.9.self_attn.out_proj.bias', 'Transf1_s.encoder.layers.9.linear1.weight', 'Transf1_s.encoder.layers.9.linear1.bias', 'Transf1_s.encoder.layers.9.linear2.weight', 'Transf1_s.encoder.layers.9.linear2.bias', 'Transf1_s.encoder.layers.9.norm1.weight', 'Transf1_s.encoder.layers.9.norm1.bias', 'Transf1_s.encoder.layers.9.norm2.weight', 'Transf1_s.encoder.layers.9.norm2.bias', 'Transf1_s.fc1.weight', 'Transf1_s.fc1.bias', 'dwconv1_t.weight', 'dwconv1_t.bias', 'conv2_t.weight', 'PosEnc1_t.pe', 'Transf1_t.encoderLayer.self_attn.in_proj_weight', 'Transf1_t.encoderLayer.self_attn.in_proj_bias', 'Transf1_t.encoderLayer.self_attn.out_proj.weight', 'Transf1_t.encoderLayer.self_attn.out_proj.bias', 'Transf1_t.encoderLayer.linear1.weight', 'Transf1_t.encoderLayer.linear1.bias', 'Transf1_t.encoderLayer.linear2.weight', 'Transf1_t.encoderLayer.linear2.bias', 'Transf1_t.encoderLayer.norm1.weight', 'Transf1_t.encoderLayer.norm1.bias', 'Transf1_t.encoderLayer.norm2.weight', 'Transf1_t.encoderLayer.norm2.bias', 'Transf1_t.encoder.layers.0.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.0.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.0.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.0.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.0.linear1.weight', 'Transf1_t.encoder.layers.0.linear1.bias', 'Transf1_t.encoder.layers.0.linear2.weight', 'Transf1_t.encoder.layers.0.linear2.bias', 'Transf1_t.encoder.layers.0.norm1.weight', 'Transf1_t.encoder.layers.0.norm1.bias', 'Transf1_t.encoder.layers.0.norm2.weight', 'Transf1_t.encoder.layers.0.norm2.bias', 'Transf1_t.encoder.layers.1.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.1.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.1.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.1.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.1.linear1.weight', 'Transf1_t.encoder.layers.1.linear1.bias', 'Transf1_t.encoder.layers.1.linear2.weight', 'Transf1_t.encoder.layers.1.linear2.bias', 'Transf1_t.encoder.layers.1.norm1.weight', 'Transf1_t.encoder.layers.1.norm1.bias', 'Transf1_t.encoder.layers.1.norm2.weight', 'Transf1_t.encoder.layers.1.norm2.bias', 'Transf1_t.encoder.layers.2.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.2.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.2.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.2.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.2.linear1.weight', 'Transf1_t.encoder.layers.2.linear1.bias', 'Transf1_t.encoder.layers.2.linear2.weight', 'Transf1_t.encoder.layers.2.linear2.bias', 'Transf1_t.encoder.layers.2.norm1.weight', 'Transf1_t.encoder.layers.2.norm1.bias', 'Transf1_t.encoder.layers.2.norm2.weight', 'Transf1_t.encoder.layers.2.norm2.bias', 'Transf1_t.encoder.layers.3.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.3.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.3.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.3.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.3.linear1.weight', 'Transf1_t.encoder.layers.3.linear1.bias', 'Transf1_t.encoder.layers.3.linear2.weight', 'Transf1_t.encoder.layers.3.linear2.bias', 'Transf1_t.encoder.layers.3.norm1.weight', 'Transf1_t.encoder.layers.3.norm1.bias', 'Transf1_t.encoder.layers.3.norm2.weight', 'Transf1_t.encoder.layers.3.norm2.bias', 'Transf1_t.encoder.layers.4.self_attn.in_proj_weight', 'Transf1_t.encoder.layers.4.self_attn.in_proj_bias', 'Transf1_t.encoder.layers.4.self_attn.out_proj.weight', 'Transf1_t.encoder.layers.4.self_attn.out_proj.bias', 'Transf1_t.encoder.layers.4.linear1.weight', 'Transf1_t.encoder.layers.4.linear1.bias', 'Transf1_t.encoder.layers.4.linear2.weight', 'Transf1_t.encoder.layers.4.linear2.bias', 'Transf1_t.encoder.layers.4.norm1.weight', 'Transf1_t.encoder.layers.4.norm1.bias', 'Transf1_t.encoder.layers.4.norm2.weight', 'Transf1_t.encoder.layers.4.norm2.bias', 'Transf1_t.fc1.weight', 'Transf1_t.fc1.bias', 'fc1.weight', 'fc1.bias'])\n","output_type":"stream"},{"execution_count":474,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# REPORT ACCURACY\ntest_preds = []\nlabels = []\nfor i, sample in enumerate(testloader):\n    accuracy = list()\n    eegTensor = sample['eeg']\n    #print(np.shape(eegTensor))\n    label = sample['label']\n    labels.append(label.detach().cpu().numpy())\n    #print(np.shape(labels))\n    #print(np.shape(label))\n    #print('sample: ', sample)\n    \n    #print(test_label)\n    eegpt.eval()\n    if cuda:\n        #print(eegTensor.shape)\n        test_pred = eegpt(eegTensor.cuda())\n        #print(test_pred.shape)\n        test_preds.append(test_pred.detach().cpu().numpy())\n        # tpred = test_pred.detach().numpy()\n        # tlabels = test_label.detach().numpy()\n        # tpredictions = get_predicted_labels(tpred)\n        #print(tpred)x\n        #accuracy.append(acc)\n    else:\n        pass\n    #print(np.mean(accuracy))\n    #Acc = np.mean(accuracy)\n\n# print('EEGPT accuracy: ',accuracy_score(tlabels,tpredictions)) # BUILD ACCURACY SCORE FUN\n# CONFUSION MATRIX\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vry23LqWX4Xw","outputId":"92fe40cf-10d3-4dbe-db1d-f79cd298c264","execution":{"iopub.status.busy":"2023-04-20T03:03:31.648934Z","iopub.execute_input":"2023-04-20T03:03:31.649437Z","iopub.status.idle":"2023-04-20T03:03:33.712367Z","shell.execute_reply.started":"2023-04-20T03:03:31.649396Z","shell.execute_reply":"2023-04-20T03:03:33.711292Z"},"trusted":true},"execution_count":475,"outputs":[]},{"cell_type":"code","source":"print(np.shape(labels[0]))\n# print(np.shape(test_preds[1]))\nprint(np.shape(test_preds[0]))\n# st_shap = np.shape(test_preds)\nprint(np.exp(test_preds[1][0]))\n#print(labels[2][5])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K31JgGBcfMs-","outputId":"a2f5276b-f4df-46b6-b157-1e587a0f2281","execution":{"iopub.status.busy":"2023-04-20T03:03:33.717667Z","iopub.execute_input":"2023-04-20T03:03:33.717983Z","iopub.status.idle":"2023-04-20T03:03:33.725541Z","shell.execute_reply.started":"2023-04-20T03:03:33.717953Z","shell.execute_reply":"2023-04-20T03:03:33.724347Z"},"trusted":true},"execution_count":476,"outputs":[{"name":"stdout","text":"(1, 1)\n(1, 5, 1)\n[[2.7883330e-01]\n [7.2108680e-01]\n [3.5423043e-05]\n [2.1946058e-05]\n [2.2553048e-05]]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_labels = list()\nfor i in range(np.shape(test_preds)[0]):\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(j)\n        class_pred = np.argmax(test_preds[i][j])\n        #print(class_pred)\n        pred_labels.append(class_pred) \nprint(np.shape(pred_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T03:03:33.727425Z","iopub.execute_input":"2023-04-20T03:03:33.728442Z","iopub.status.idle":"2023-04-20T03:03:33.738742Z","shell.execute_reply.started":"2023-04-20T03:03:33.728380Z","shell.execute_reply":"2023-04-20T03:03:33.737632Z"},"trusted":true},"execution_count":477,"outputs":[{"name":"stdout","text":"(142,)\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = list()\nfor i in range(np.shape(labels)[0]):\n    # print(i)\n    for j in range(np.shape(test_preds[i])[0]):\n        # print(np.exp(test_preds[i][1]))\n        #print(labels[j][0])\n        #print(class_pred)\n        true_labels.append(labels[i][j]) \nprint(np.shape(true_labels))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T03:03:33.740443Z","iopub.execute_input":"2023-04-20T03:03:33.741343Z","iopub.status.idle":"2023-04-20T03:03:33.751476Z","shell.execute_reply.started":"2023-04-20T03:03:33.741294Z","shell.execute_reply":"2023-04-20T03:03:33.750400Z"},"trusted":true},"execution_count":478,"outputs":[{"name":"stdout","text":"(142, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"brk = len(true_labels)\nCM = confusion_matrix(true_labels[1:brk], pred_labels[1:brk])\naccuracy = accuracy_score(true_labels[1:brk], pred_labels[1:brk])\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10}, fmt='d')\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');\nprint('accuracy: ',accuracy)","metadata":{"id":"-JcC_9tief8C","execution":{"iopub.status.busy":"2023-04-20T03:03:33.753227Z","iopub.execute_input":"2023-04-20T03:03:33.753978Z","iopub.status.idle":"2023-04-20T03:03:34.046054Z","shell.execute_reply.started":"2023-04-20T03:03:33.753940Z","shell.execute_reply":"2023-04-20T03:03:34.044895Z"},"trusted":true},"execution_count":479,"outputs":[{"name":"stdout","text":"accuracy:  0.723404255319149\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"#@title TABS REFERENCE\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n# Can add squeeze excitation layers if you want to try that as well.\nclass ChannelSELayer3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block described in:\n        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=8):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ChannelSELayer3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n        # Average along each channel\n        squeeze_tensor = self.avg_pool(input_tensor)\n\n        # channel excitation\n        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n\n        output_tensor = torch.mul(input_tensor, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n\n        return output_tensor\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x\n","metadata":{"id":"MLfq9obROrbO","cellView":"form","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-20T02:20:33.144543Z","iopub.execute_input":"2023-04-20T02:20:33.145202Z","iopub.status.idle":"2023-04-20T02:20:33.206823Z","shell.execute_reply.started":"2023-04-20T02:20:33.145163Z","shell.execute_reply":"2023-04-20T02:20:33.205595Z"},"trusted":true},"execution_count":304,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}